{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11224133,"sourceType":"datasetVersion","datasetId":7010008}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/raihanewubd/selfSupervised/blob/main/i_jepa_aav_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"from google.colab import drive\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom torchvision.models import vit_b_16\n\n\n\nimport timm\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom collections import Counter\n\n\nimport random\nimport os\nimport copy\nimport time\nimport pickle\n\n","metadata":{"_uuid":"ab0f765f-50d0-4e71-934b-69036f927618","_cell_guid":"1b74e769-17ce-492e-bbf7-eb90938bae5d","id":"7Qm5o5BpuTyt","jupyter":{"outputs_hidden":false},"trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:53:41.417251Z","iopub.execute_input":"2025-04-03T06:53:41.417626Z","iopub.status.idle":"2025-04-03T06:53:41.423272Z","shell.execute_reply.started":"2025-04-03T06:53:41.417590Z","shell.execute_reply":"2025-04-03T06:53:41.422312Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#drive.mount('/content/drive')","metadata":{"_uuid":"b02bcb6e-1d67-4b0c-9d11-a22f3ab07a61","_cell_guid":"d0758325-ed5a-4988-b20a-51539395a900","trusted":true,"id":"Gxnc6znruQbZ","outputId":"9a7e3c98-b215-41a6-da85-f0ccbf58bc80","jupyter":{"outputs_hidden":false},"colab":{"base_uri":"https://localhost:8080/"},"collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:53:41.424400Z","iopub.execute_input":"2025-04-03T06:53:41.424721Z","iopub.status.idle":"2025-04-03T06:53:41.440830Z","shell.execute_reply.started":"2025-04-03T06:53:41.424693Z","shell.execute_reply":"2025-04-03T06:53:41.440090Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define base directory and file name for saving the classifier checkpoint.\nbase_dir = \"/kaggle/working\"\n#base_dir = \"/content/drive/MyDrive/AAVDATASET/spectrogram\"","metadata":{"id":"51MrUfzzXm_a","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:53:41.442584Z","iopub.execute_input":"2025-04-03T06:53:41.442855Z","iopub.status.idle":"2025-04-03T06:53:41.456191Z","shell.execute_reply.started":"2025-04-03T06:53:41.442834Z","shell.execute_reply":"2025-04-03T06:53:41.455450Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data_dir = '/kaggle/input/aav-spectrogram/spectrogram'\n#data_dir = os.path.join(base_dir,'dataset')\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"_uuid":"2bb0d814-c433-4069-b2de-b8afb06f8c33","_cell_guid":"cfbd3054-08ff-4e37-a1dd-a9816ded260d","trusted":true,"id":"fKthbkn-t67J","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:53:41.457201Z","iopub.execute_input":"2025-04-03T06:53:41.457495Z","iopub.status.idle":"2025-04-03T06:53:42.624613Z","shell.execute_reply.started":"2025-04-03T06:53:41.457472Z","shell.execute_reply":"2025-04-03T06:53:42.623849Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Set device for GPU acceleration if available.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zIe8gpxxU60","outputId":"2047897e-2614-44a2-e53b-a29879c16e7f","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:53:42.625479Z","iopub.execute_input":"2025-04-03T06:53:42.625684Z","iopub.status.idle":"2025-04-03T06:53:42.629767Z","shell.execute_reply.started":"2025-04-03T06:53:42.625666Z","shell.execute_reply":"2025-04-03T06:53:42.628903Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def extract_blocks(image, context_scale=0.85, target_scale=0.2, num_targets=4, max_overlap=0.5):\n    # Extract a central context block.\n    _, H, W = image.shape\n    context_size = int(context_scale * H)\n    top = (H - context_size) // 2\n    left = (W - context_size) // 2\n    context_block = image[:, top:top+context_size, left:left+context_size]\n    context_block = torch.nn.functional.interpolate(\n        context_block.unsqueeze(0),\n        size=(224, 224),\n        mode='bilinear',\n        align_corners=False\n    ).squeeze(0)\n\n    # Extract num_targets target blocks randomly.\n    target_blocks = []\n    for _ in range(num_targets):\n        target_size = int(target_scale * H)\n        top_t = random.randint(0, H - target_size)\n        left_t = random.randint(0, W - target_size)\n        target_block = image[:, top_t:top_t+target_size, left_t:left_t+target_size]\n        target_block = torch.nn.functional.interpolate(\n            target_block.unsqueeze(0),\n            size=(224, 224),\n            mode='bilinear',\n            align_corners=False\n        ).squeeze(0)\n        target_blocks.append(target_block)\n    target_blocks = torch.stack(target_blocks)\n    return context_block, target_blocks, (top, left, context_size), None\n\n","metadata":{"id":"cuurpOHkxbxk","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:53:42.630550Z","iopub.execute_input":"2025-04-03T06:53:42.630785Z","iopub.status.idle":"2025-04-03T06:53:42.645337Z","shell.execute_reply.started":"2025-04-03T06:53:42.630757Z","shell.execute_reply":"2025-04-03T06:53:42.644542Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def process_sample(sample, context_scale, target_scale, num_targets):\n    # Unpack sample: sample is ((img, label), image_path)\n    (img, label), image_path = sample\n    # Move image to GPU if available.\n    img = img.to(device)\n    context_block, target_blocks, _, _ = extract_blocks(img, context_scale, target_scale, num_targets)\n    # Bring results back to CPU before caching.\n    return (context_block.cpu(), target_blocks.cpu(), label, image_path)","metadata":{"id":"oj2gcdgq4hvd","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:53:42.646174Z","iopub.execute_input":"2025-04-03T06:53:42.646454Z","iopub.status.idle":"2025-04-03T06:53:42.659788Z","shell.execute_reply.started":"2025-04-03T06:53:42.646426Z","shell.execute_reply":"2025-04-03T06:53:42.659060Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class PrecomputedIJEPADataset(Dataset):\n    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4, cache_file=None):\n        self.cache_file = cache_file\n        if cache_file and os.path.exists(cache_file):\n            # Load precomputed data from disk.\n            with open(cache_file, 'rb') as f:\n                self.data = pickle.load(f)\n        else:\n            # Create a list of samples along with their original image paths (if available) using a progress bar.\n            if hasattr(base_dataset, 'samples'):\n                base_samples = [\n                    (base_dataset[i], base_dataset.samples[i][0])\n                    for i in tqdm(range(len(base_dataset)), desc=\"Loading samples\")\n                ]\n            else:\n                base_samples = [\n                    (sample, None) for sample in tqdm(base_dataset, desc=\"Loading samples\")\n                ]\n\n            # Process samples sequentially with a progress bar.\n            self.data = []\n            for sample in tqdm(base_samples, desc=\"Processing samples\"):\n                result = process_sample(sample, context_scale, target_scale, num_targets)\n                self.data.append(result)\n\n            if cache_file:\n                with open(cache_file, 'wb') as f:\n                    pickle.dump(self.data, f)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n","metadata":{"id":"w3876qql71lv","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:53:42.660595Z","iopub.execute_input":"2025-04-03T06:53:42.660891Z","iopub.status.idle":"2025-04-03T06:53:42.677098Z","shell.execute_reply.started":"2025-04-03T06:53:42.660861Z","shell.execute_reply":"2025-04-03T06:53:42.676332Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\n\n# Timing the loading of the dataset and DataLoader\ncache_path = os.path.join(base_dir,\"precomputed_fulldataset_aav.pkl\")\nprint(cache_path)\nstart_time = time.time()\ndataset_aav_ijepa = PrecomputedIJEPADataset(dataset, cache_file=cache_path)\nend_time_train_ijepa_dataset = time.time()\ndataloader_aav_ijepa = DataLoader(dataset_aav_ijepa, batch_size=32, shuffle=True)\nend_time = time.time()\nprint(f\"Time taken to load dataset: {end_time_train_ijepa_dataset - start_time:.4f} seconds and DataLoader: {end_time - end_time_train_ijepa_dataset:.4f} seconds\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54OBcd7SgFde","outputId":"bfa2b185-0136-43ab-a541-410d9f5ae95d","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:53:42.679671Z","iopub.execute_input":"2025-04-03T06:53:42.679942Z","iopub.status.idle":"2025-04-03T06:55:06.988604Z","shell.execute_reply.started":"2025-04-03T06:53:42.679911Z","shell.execute_reply":"2025-04-03T06:55:06.987460Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/precomputed_fulldataset_aav.pkl\n","output_type":"stream"},{"name":"stderr","text":"Loading samples: 100%|██████████| 3513/3513 [00:35<00:00, 97.78it/s] \nProcessing samples: 100%|██████████| 3513/3513 [00:12<00:00, 283.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Time taken to load dataset: 84.2955 seconds and DataLoader: 0.0014 seconds\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"num_images = len(dataset_aav_ijepa)\nprint(f\"Number of images in the dataset: {num_images}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEaXCJIKJLBt","outputId":"9a487a6b-18c4-4478-bc20-da5f7efff64a","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:06.990393Z","iopub.execute_input":"2025-04-03T06:55:06.990634Z","iopub.status.idle":"2025-04-03T06:55:06.994704Z","shell.execute_reply.started":"2025-04-03T06:55:06.990614Z","shell.execute_reply":"2025-04-03T06:55:06.993778Z"}},"outputs":[{"name":"stdout","text":"Number of images in the dataset: 3513\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"total_batches = len(dataloader_aav_ijepa)\nprint(\"Total number of batches:\", total_batches)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pXBRPzuZPZH","outputId":"bd63ba50-dbe3-4e88-f815-fe4587a2d7f4","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:06.995584Z","iopub.execute_input":"2025-04-03T06:55:06.995907Z","iopub.status.idle":"2025-04-03T06:55:08.039526Z","shell.execute_reply.started":"2025-04-03T06:55:06.995877Z","shell.execute_reply":"2025-04-03T06:55:08.038390Z"}},"outputs":[{"name":"stdout","text":"Total number of batches: 110\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"'''import torch\nfrom torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\n\n\n# Define the split ratio (e.g., 80% train, 20% test)\ntrain_ratio = 0.8\ntest_ratio = 1 - train_ratio\n\n# Get the total number of samples in the dataset.\nnum_samples = len(dataset_aav_ijepa)\n\n# Create a list of indices for all samples in the dataset.\nindices = list(range(num_samples))\n\n# Split the indices into train and test sets using train_test_split.\ntrain_indices, test_indices = train_test_split(indices, test_size=test_ratio, random_state=42)  # Set random_state for reproducibility.\n\n# Create Subset datasets for train and test using the split indices.\ntrain_dataset_aav_ijepa = Subset(dataset_aav_ijepa, train_indices)\ntest_dataset_aav_ijepa = Subset(dataset_aav_ijepa, test_indices)\n\n# Create DataLoaders for the train and test datasets.\ntrain_loader_aav_ijepa = torch.utils.data.DataLoader(train_dataset_aav_ijepa, batch_size=32, shuffle=True)\ntest_loader_aav_ijepa = torch.utils.data.DataLoader(test_dataset_aav_ijepa, batch_size=32, shuffle=False)  # No need to shuffle the test set.\n\nprint(f\"Training set size: {len(train_dataset_aav_ijepa)}\")\nprint(f\"Testing set size: {len(test_dataset_aav_ijepa)}\")'''","metadata":{"id":"EqNu-u2qLX44","outputId":"24f917f4-1e58-4f72-d50c-7ff688eac560","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:08.040388Z","iopub.execute_input":"2025-04-03T06:55:08.040640Z","iopub.status.idle":"2025-04-03T06:55:09.037692Z","shell.execute_reply.started":"2025-04-03T06:55:08.040614Z","shell.execute_reply":"2025-04-03T06:55:09.036960Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'import torch\\nfrom torch.utils.data import Subset\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n# Define the split ratio (e.g., 80% train, 20% test)\\ntrain_ratio = 0.8\\ntest_ratio = 1 - train_ratio\\n\\n# Get the total number of samples in the dataset.\\nnum_samples = len(dataset_aav_ijepa)\\n\\n# Create a list of indices for all samples in the dataset.\\nindices = list(range(num_samples))\\n\\n# Split the indices into train and test sets using train_test_split.\\ntrain_indices, test_indices = train_test_split(indices, test_size=test_ratio, random_state=42)  # Set random_state for reproducibility.\\n\\n# Create Subset datasets for train and test using the split indices.\\ntrain_dataset_aav_ijepa = Subset(dataset_aav_ijepa, train_indices)\\ntest_dataset_aav_ijepa = Subset(dataset_aav_ijepa, test_indices)\\n\\n# Create DataLoaders for the train and test datasets.\\ntrain_loader_aav_ijepa = torch.utils.data.DataLoader(train_dataset_aav_ijepa, batch_size=32, shuffle=True)\\ntest_loader_aav_ijepa = torch.utils.data.DataLoader(test_dataset_aav_ijepa, batch_size=32, shuffle=False)  # No need to shuffle the test set.\\n\\nprint(f\"Training set size: {len(train_dataset_aav_ijepa)}\")\\nprint(f\"Testing set size: {len(test_dataset_aav_ijepa)}\")'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Subset, random_split\nfrom sklearn.model_selection import train_test_split\n\n# Define the split ratios (e.g., 70% train, 15% validation, 15% test)\ntrain_ratio = 0.7\nval_ratio = 0.15\ntest_ratio = 0.15\n\n# Get the total number of samples in the dataset.\nnum_samples = len(dataset_aav_ijepa)\n\n# 1. Split into train and (validation + test)\ntrain_indices, val_test_indices = train_test_split(\n    list(range(num_samples)),\n    test_size=val_ratio + test_ratio,\n    random_state=42  # Set random_state for reproducibility\n)\n\n# 2. Split (validation + test) into validation and test\nval_indices, test_indices = train_test_split(\n    val_test_indices,\n    test_size=test_ratio / (val_ratio + test_ratio),\n    random_state=42  # Set random_state for reproducibility\n)\n\n# Create Subset datasets for train, validation, and test\ntrain_dataset_aav_ijepa = Subset(dataset_aav_ijepa, train_indices)\nval_dataset_aav_ijepa = Subset(dataset_aav_ijepa, val_indices)\ntest_dataset_aav_ijepa = Subset(dataset_aav_ijepa, test_indices)\n\n# Create DataLoaders for train, validation, and test\ntrain_loader_aav_ijepa = torch.utils.data.DataLoader(train_dataset_aav_ijepa, batch_size=32, shuffle=True)\nval_loader_aav_ijepa = torch.utils.data.DataLoader(val_dataset_aav_ijepa, batch_size=32, shuffle=False)\ntest_loader_aav_ijepa = torch.utils.data.DataLoader(test_dataset_aav_ijepa, batch_size=32, shuffle=False)\n\nprint(f\"Training set size: {len(train_dataset_aav_ijepa)}\")\nprint(f\"Validation set size: {len(val_dataset_aav_ijepa)}\")\nprint(f\"Testing set size: {len(test_dataset_aav_ijepa)}\")","metadata":{"id":"YcNSpd5mLi3g","outputId":"3ff04da4-5c3f-40f6-e0c7-fe0794fa41cb","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:09.038558Z","iopub.execute_input":"2025-04-03T06:55:09.038908Z","iopub.status.idle":"2025-04-03T06:55:16.700577Z","shell.execute_reply.started":"2025-04-03T06:55:09.038876Z","shell.execute_reply":"2025-04-03T06:55:16.699850Z"}},"outputs":[{"name":"stdout","text":"Training set size: 2459\nValidation set size: 527\nTesting set size: 527\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def get_vit_encoder():\n    model = vit_b_16(pretrained=False)\n    model.heads = nn.Identity()  # remove classification head\n    return model","metadata":{"_uuid":"f3ad3daa-80ba-4ae8-8b4e-770815bad02e","_cell_guid":"7aeb9d44-2fd8-4da1-87c0-4b992adb53db","trusted":true,"jupyter":{"outputs_hidden":false},"id":"YQIK4mND8qK1","collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:55:16.701458Z","iopub.execute_input":"2025-04-03T06:55:16.701771Z","iopub.status.idle":"2025-04-03T06:55:16.705295Z","shell.execute_reply.started":"2025-04-03T06:55:16.701739Z","shell.execute_reply":"2025-04-03T06:55:16.704596Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\ncontext_encoder = get_vit_encoder().cuda()\ntarget_encoder  = get_vit_encoder().cuda()\ntarget_encoder.load_state_dict(context_encoder.state_dict())","metadata":{"_uuid":"b79e21b1-3b97-4dfc-958f-bddb3c94c746","_cell_guid":"2d8198b7-dbf5-4228-975a-8c733cea1a07","trusted":true,"jupyter":{"outputs_hidden":false},"id":"rkq_fT7g8qK1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"937b6613-1e96-4188-9910-59f4a58f9004","collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:55:16.706049Z","iopub.execute_input":"2025-04-03T06:55:16.706263Z","iopub.status.idle":"2025-04-03T06:55:19.521907Z","shell.execute_reply.started":"2025-04-03T06:55:16.706244Z","shell.execute_reply":"2025-04-03T06:55:19.521097Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"class Predictor(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, output_dim=768, num_targets=4):\n        super().__init__()\n        self.num_targets = num_targets\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim * num_targets)\n        )\n    def forward(self, context_repr):\n        pred = self.mlp(context_repr)\n        # Reshape to [B, num_targets, output_dim]\n        return pred.view(-1, self.num_targets, pred.size(-1) // self.num_targets)","metadata":{"_uuid":"e303f4ac-a09b-4f56-b997-6b364d63a675","_cell_guid":"9206cc42-0845-4604-ab30-6852e28e2dcb","trusted":true,"jupyter":{"outputs_hidden":false},"id":"OApD7Fy68qK1","collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.522620Z","iopub.execute_input":"2025-04-03T06:55:19.522847Z","iopub.status.idle":"2025-04-03T06:55:19.527699Z","shell.execute_reply.started":"2025-04-03T06:55:19.522830Z","shell.execute_reply":"2025-04-03T06:55:19.526866Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# 6. Set up optimizer, loss, and EMA update (same as your CIFAR code).\npredictor = Predictor().cuda()\noptimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=1e-1)\ncriterion = nn.MSELoss()\nema_decay = 0.99","metadata":{"_uuid":"66d2337f-fb52-4652-9da7-242ce9d9d5e1","_cell_guid":"e10ef302-1659-4a97-9bbb-2de90ac902c9","trusted":true,"jupyter":{"outputs_hidden":false},"id":"ZsZNIdEk8qK2","collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.528423Z","iopub.execute_input":"2025-04-03T06:55:19.528602Z","iopub.status.idle":"2025-04-03T06:55:19.573007Z","shell.execute_reply.started":"2025-04-03T06:55:19.528586Z","shell.execute_reply":"2025-04-03T06:55:19.572438Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"@torch.no_grad()\ndef update_ema(model, model_ema, beta):\n    for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n        param_ema.data.mul_(beta).add_(param.data, alpha=1 - beta)","metadata":{"_uuid":"0e90599a-d03d-4231-a58a-1272a3613cc3","_cell_guid":"aece458a-d1da-4334-804f-b054cf8ed0fc","trusted":true,"jupyter":{"outputs_hidden":false},"id":"kM60lfOw8qK2","collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.573669Z","iopub.execute_input":"2025-04-03T06:55:19.573897Z","iopub.status.idle":"2025-04-03T06:55:19.577684Z","shell.execute_reply.started":"2025-04-03T06:55:19.573877Z","shell.execute_reply":"2025-04-03T06:55:19.576864Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"sample = next(iter(train_dataset_aav_ijepa))\nfor i, item in enumerate(sample):\n    print(f\"Item {i}: shape = {item.shape if hasattr(item, 'shape') else type(item)}\")\n    print(f\"Item {i}: shape = {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:59:53.676709Z","iopub.execute_input":"2025-04-03T06:59:53.677104Z","iopub.status.idle":"2025-04-03T06:59:53.720882Z","shell.execute_reply.started":"2025-04-03T06:59:53.677078Z","shell.execute_reply":"2025-04-03T06:59:53.720006Z"}},"outputs":[{"name":"stdout","text":"Item 0: shape = torch.Size([3, 224, 224])\nItem 0: shape = tensor([[[-0.5137, -0.5137, -0.5137,  ..., -0.5294, -0.5294, -0.5294],\n         [-0.5137, -0.5137, -0.5100,  ..., -0.5294, -0.5294, -0.5294],\n         [-0.5089, -0.5089, -0.5070,  ..., -0.5197, -0.5197, -0.5197],\n         ...,\n         [-0.3569, -0.3569, -0.3569,  ..., -0.3155, -0.3155, -0.3155],\n         [-0.3926, -0.3926, -0.3926,  ..., -0.3327, -0.3327, -0.3327],\n         [-0.5137, -0.5137, -0.5137,  ..., -0.4902, -0.4902, -0.4902]],\n\n        [[ 0.4510,  0.4510,  0.4510,  ...,  0.4118,  0.4118,  0.4118],\n         [ 0.4510,  0.4510,  0.4510,  ...,  0.3671,  0.3633,  0.3633],\n         [ 0.4558,  0.4558,  0.4558,  ...,  0.4141,  0.4123,  0.4123],\n         ...,\n         [ 0.5245,  0.5245,  0.5245,  ...,  0.5383,  0.5383,  0.5383],\n         [ 0.5116,  0.5116,  0.5116,  ...,  0.5315,  0.5315,  0.5315],\n         [ 0.4510,  0.4510,  0.4510,  ...,  0.4588,  0.4588,  0.4588]],\n\n        [[-0.1373, -0.1373, -0.1373,  ..., -0.1059, -0.1059, -0.1059],\n         [-0.1312, -0.1312, -0.1312,  ..., -0.0938, -0.0938, -0.0938],\n         [-0.1343, -0.1343, -0.1343,  ..., -0.1145, -0.1145, -0.1145],\n         ...,\n         [-0.2157, -0.2157, -0.2157,  ..., -0.2433, -0.2444, -0.2481],\n         [-0.1978, -0.1978, -0.1978,  ..., -0.2360, -0.2360, -0.2360],\n         [-0.1373, -0.1373, -0.1373,  ..., -0.1451, -0.1451, -0.1451]]])\nItem 1: shape = torch.Size([4, 3, 224, 224])\nItem 1: shape = tensor([[[[-0.5373, -0.5373, -0.5373,  ..., -0.5451, -0.5451, -0.5451],\n          [-0.5373, -0.5373, -0.5373,  ..., -0.5451, -0.5451, -0.5451],\n          [-0.5373, -0.5373, -0.5373,  ..., -0.5451, -0.5451, -0.5451],\n          ...,\n          [-0.4980, -0.4980, -0.4980,  ..., -0.4902, -0.4902, -0.4902],\n          [-0.4980, -0.4980, -0.4980,  ..., -0.4902, -0.4902, -0.4902],\n          [-0.4980, -0.4980, -0.4980,  ..., -0.4902, -0.4902, -0.4902]],\n\n         [[ 0.4353,  0.4353,  0.4353,  ...,  0.4275,  0.4275,  0.4275],\n          [ 0.4353,  0.4353,  0.4353,  ...,  0.4275,  0.4275,  0.4275],\n          [ 0.4353,  0.4353,  0.4353,  ...,  0.4275,  0.4275,  0.4275],\n          ...,\n          [ 0.4510,  0.4510,  0.4510,  ...,  0.4588,  0.4588,  0.4588],\n          [ 0.4510,  0.4510,  0.4510,  ...,  0.4588,  0.4588,  0.4588],\n          [ 0.4510,  0.4510,  0.4510,  ...,  0.4588,  0.4588,  0.4588]],\n\n         [[-0.1059, -0.1059, -0.1059,  ..., -0.1059, -0.1059, -0.1059],\n          [-0.1059, -0.1059, -0.1059,  ..., -0.1059, -0.1059, -0.1059],\n          [-0.1059, -0.1059, -0.1059,  ..., -0.1059, -0.1059, -0.1059],\n          ...,\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1451, -0.1451, -0.1451],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1451, -0.1451, -0.1451],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1451, -0.1451, -0.1451]]],\n\n\n        [[[-0.5373, -0.5373, -0.5373,  ..., -0.5765, -0.5765, -0.5765],\n          [-0.5373, -0.5373, -0.5373,  ..., -0.5765, -0.5765, -0.5765],\n          [-0.5373, -0.5373, -0.5373,  ..., -0.5765, -0.5765, -0.5765],\n          ...,\n          [-0.4588, -0.4588, -0.4588,  ..., -0.4431, -0.4431, -0.4431],\n          [-0.4588, -0.4588, -0.4588,  ..., -0.4431, -0.4431, -0.4431],\n          [-0.4588, -0.4588, -0.4588,  ..., -0.4431, -0.4431, -0.4431]],\n\n         [[ 0.4275,  0.4275,  0.4275,  ...,  0.3961,  0.3961,  0.3961],\n          [ 0.4275,  0.4275,  0.4275,  ...,  0.3961,  0.3961,  0.3961],\n          [ 0.4275,  0.4275,  0.4275,  ...,  0.3961,  0.3961,  0.3961],\n          ...,\n          [ 0.4745,  0.4745,  0.4745,  ...,  0.4902,  0.4902,  0.4902],\n          [ 0.4745,  0.4745,  0.4745,  ...,  0.4902,  0.4902,  0.4902],\n          [ 0.4745,  0.4745,  0.4745,  ...,  0.4902,  0.4902,  0.4902]],\n\n         [[-0.1216, -0.1216, -0.1216,  ..., -0.0980, -0.0980, -0.0980],\n          [-0.1216, -0.1216, -0.1216,  ..., -0.0980, -0.0980, -0.0980],\n          [-0.1216, -0.1216, -0.1216,  ..., -0.0980, -0.0980, -0.0980],\n          ...,\n          [-0.1608, -0.1608, -0.1608,  ..., -0.1686, -0.1686, -0.1686],\n          [-0.1608, -0.1608, -0.1608,  ..., -0.1686, -0.1686, -0.1686],\n          [-0.1608, -0.1608, -0.1608,  ..., -0.1686, -0.1686, -0.1686]]],\n\n\n        [[[-0.5059, -0.5059, -0.5059,  ..., -0.5137, -0.5137, -0.5137],\n          [-0.5059, -0.5059, -0.5059,  ..., -0.5137, -0.5137, -0.5137],\n          [-0.5059, -0.5059, -0.5059,  ..., -0.5137, -0.5137, -0.5137],\n          ...,\n          [-0.4745, -0.4745, -0.4745,  ..., -0.4353, -0.4353, -0.4353],\n          [-0.4745, -0.4745, -0.4745,  ..., -0.4353, -0.4353, -0.4353],\n          [-0.4745, -0.4745, -0.4745,  ..., -0.4353, -0.4353, -0.4353]],\n\n         [[ 0.4588,  0.4588,  0.4588,  ...,  0.4510,  0.4510,  0.4510],\n          [ 0.4588,  0.4588,  0.4588,  ...,  0.4510,  0.4510,  0.4510],\n          [ 0.4588,  0.4588,  0.4588,  ...,  0.4510,  0.4510,  0.4510],\n          ...,\n          [ 0.4667,  0.4667,  0.4667,  ...,  0.4902,  0.4902,  0.4902],\n          [ 0.4667,  0.4667,  0.4667,  ...,  0.4902,  0.4902,  0.4902],\n          [ 0.4667,  0.4667,  0.4667,  ...,  0.4902,  0.4902,  0.4902]],\n\n         [[-0.1373, -0.1373, -0.1373,  ..., -0.1294, -0.1294, -0.1294],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1294, -0.1294, -0.1294],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1294, -0.1294, -0.1294],\n          ...,\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1686, -0.1686, -0.1686],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1686, -0.1686, -0.1686],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1686, -0.1686, -0.1686]]],\n\n\n        [[[-0.4667, -0.4667, -0.4667,  ..., -0.4902, -0.4902, -0.4902],\n          [-0.4667, -0.4667, -0.4667,  ..., -0.4902, -0.4902, -0.4902],\n          [-0.4667, -0.4667, -0.4667,  ..., -0.4902, -0.4902, -0.4902],\n          ...,\n          [-0.4980, -0.4980, -0.4980,  ..., -0.5294, -0.5294, -0.5294],\n          [-0.4980, -0.4980, -0.4980,  ..., -0.5294, -0.5294, -0.5294],\n          [-0.4980, -0.4980, -0.4980,  ..., -0.5294, -0.5294, -0.5294]],\n\n         [[ 0.4745,  0.4745,  0.4745,  ...,  0.4588,  0.4588,  0.4588],\n          [ 0.4745,  0.4745,  0.4745,  ...,  0.4588,  0.4588,  0.4588],\n          [ 0.4745,  0.4745,  0.4745,  ...,  0.4588,  0.4588,  0.4588],\n          ...,\n          [ 0.4275,  0.4275,  0.4275,  ...,  0.4196,  0.4196,  0.4196],\n          [ 0.4275,  0.4275,  0.4275,  ...,  0.4196,  0.4196,  0.4196],\n          [ 0.4275,  0.4275,  0.4275,  ...,  0.4196,  0.4196,  0.4196]],\n\n         [[-0.1529, -0.1529, -0.1529,  ..., -0.1451, -0.1451, -0.1451],\n          [-0.1529, -0.1529, -0.1529,  ..., -0.1451, -0.1451, -0.1451],\n          [-0.1529, -0.1529, -0.1529,  ..., -0.1451, -0.1451, -0.1451],\n          ...,\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1216, -0.1216, -0.1216],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1216, -0.1216, -0.1216],\n          [-0.1373, -0.1373, -0.1373,  ..., -0.1216, -0.1216, -0.1216]]]])\nItem 2: shape = <class 'int'>\nItem 2: shape = 2\nItem 3: shape = <class 'str'>\nItem 3: shape = /kaggle/input/aav-spectrogram/spectrogram/single/segment_523.png\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Create a directory for visualizations if it doesn't exist.\n#viz_dir = \"/kaggle/working/viz\"\n#os.makedirs(viz_dir, exist_ok=True)\n\nnum_epochs = 5\nema_decay = 0.1\nbest_loss = float('inf')\ntotal_start_time = time.time()\n\nfor epoch in range(num_epochs):\n    epoch_start_time = time.time()\n    context_encoder.train()\n    predictor.train()\n    running_loss = 0.0\n\n    # Enumerate over batches with a progress bar.\n    for batch_idx, (context_block, target_blocks, class_label, filepath) in enumerate(tqdm(train_dataset_aav_ijepa, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n        context_block = context_block.cuda()            # [B, C, 224, 224]\n        target_blocks = target_blocks.cuda()              # [B, num_targets, C, 224, 224]\n\n        # Forward pass through context encoder and predictor.\n        context_repr = context_encoder(context_block)     # [B, 768]\n        preds = predictor(context_repr)                   # [B, num_targets, 768]\n\n        B, num_targets, C, Ht, Wt = target_blocks.shape\n        target_blocks_flat = target_blocks.view(B * num_targets, C, Ht, Wt)\n        with torch.no_grad():\n            target_repr_flat = target_encoder(target_blocks_flat)\n        target_repr = target_repr_flat.view(B, num_targets, -1)\n\n        loss = criterion(preds, target_repr)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        update_ema(context_encoder, target_encoder, ema_decay)\n        running_loss += loss.item() * context_block.size(0)\n\n        # --- Visualization for first image of the current batch ---\n        '''with torch.no_grad():\n            # Get the first sample's context block and compute its feature vector.\n            context_img = context_block[0].cpu()  # shape: [C, 224, 224]\n            context_feat = context_encoder(context_block[0].unsqueeze(0)).cpu().squeeze(0)  # shape: [768]\n            # Reshape feature vector to a 2D heatmap (24x32).\n            context_heat = context_feat.view(24, 32).numpy()\n\n            # For target, choose the first target block of the first sample.\n            target_img = target_blocks[0][0].cpu()  # shape: [C, 224, 224]\n            target_feat = target_encoder(target_blocks[0][0].unsqueeze(0).to(context_block.device)).cpu().squeeze(0)\n            target_heat = target_feat.view(24, 32).numpy()\n\n            # Plot the images and corresponding heatmaps.\n            fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n            # Display context block image.\n            if context_img.shape[0] == 1:\n                axs[0, 0].imshow(context_img.squeeze(), cmap='gray')\n            else:\n                axs[0, 0].imshow(context_img.permute(1, 2, 0))\n            axs[0, 0].set_title(\"Context Block\")\n            axs[0, 0].axis(\"off\")\n\n            # Display context feature heatmap.\n            im0 = axs[0, 1].imshow(context_heat, cmap=\"viridis\")\n            axs[0, 1].set_title(\"Context Feature Heatmap\")\n            axs[0, 1].axis(\"off\")\n            fig.colorbar(im0, ax=axs[0, 1])\n\n            # Display target block image.\n            if target_img.shape[0] == 1:\n                axs[1, 0].imshow(target_img.squeeze(), cmap='gray')\n            else:\n                axs[1, 0].imshow(target_img.permute(1, 2, 0))\n            axs[1, 0].set_title(\"Target Block\")\n            axs[1, 0].axis(\"off\")\n\n            # Display target feature heatmap.\n            im1 = axs[1, 1].imshow(target_heat, cmap=\"viridis\")\n            axs[1, 1].set_title(\"Target Feature Heatmap\")\n            axs[1, 1].axis(\"off\")\n            fig.colorbar(im1, ax=axs[1, 1])\n\n            # Save the visualization figure with epoch and batch number.\n            viz_path = os.path.join(viz_dir, f\"epoch{epoch+1}_batch{batch_idx+1}.png\")\n            plt.savefig(viz_path)\n            plt.close(fig)'''\n\n    epoch_loss = running_loss / len(train_dataset_aav_ijepa)\n    epoch_time = time.time() - epoch_start_time\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.10f} - Epoch Time: {epoch_time:.2f}s\")\n\n    # Save checkpoint if current epoch loss is lower than previous best.\n    if epoch_loss < best_loss:\n        best_loss = epoch_loss\n        checkpoint = {\n            'epoch': epoch+1,\n            'context_encoder_state_dict': context_encoder.state_dict(),\n            'target_encoder_state_dict': target_encoder.state_dict(),\n            'predictor_state_dict': predictor.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': epoch_loss\n        }\n        torch.save(checkpoint, os.path.join(base_dir,\"ijepa_checkpoint_best.pth\"))\n        print(f\"Checkpoint saved at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n\n\ntotal_train_time = time.time() - total_start_time\nprint(f\"Total Training Time: {total_train_time:.2f}s\")","metadata":{"_uuid":"c8659b2c-5286-4027-b97b-c1ac887bb5f6","_cell_guid":"0182b713-044f-4839-9cfc-0369f1eec96d","trusted":true,"jupyter":{"outputs_hidden":false},"id":"IjnBW1lE8qK2","collapsed":false,"execution":{"iopub.status.busy":"2025-04-03T07:19:39.795303Z","iopub.execute_input":"2025-04-03T07:19:39.795624Z","iopub.status.idle":"2025-04-03T07:19:39.831374Z","shell.execute_reply.started":"2025-04-03T07:19:39.795601Z","shell.execute_reply":"2025-04-03T07:19:39.830315Z"}},"outputs":[{"name":"stderr","text":"                                                   \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-a5d80477a3cc>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Forward pass through context encoder and predictor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mcontext_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_block\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [B, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_repr\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# [B, num_targets, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Reshape and permute the input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36m_process_input\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Wrong image height! Expected {self.image_size} but got {h}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"],"ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 3)","output_type":"error"}],"execution_count":33},{"cell_type":"markdown","source":"# Train the Classifier","metadata":{"id":"clZWxsbpZ9EO"}},{"cell_type":"markdown","source":"## 1. Load the Saved Checkpoint for the Self-Supervised Model","metadata":{"id":"GQnzVW9aaDL4"}},{"cell_type":"code","source":"checkpoint = torch.load(os.path.join(base_dir, \"ijepa_checkpoint_best.pth\"))\ncontext_encoder.load_state_dict(checkpoint['context_encoder_state_dict'])\n# Freeze the context encoder.\ncontext_encoder.eval()\nfor param in context_encoder.parameters():\n    param.requires_grad = False","metadata":{"id":"TXG_-3UPaHTE","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.623679Z","iopub.status.idle":"2025-04-03T06:55:19.623969Z","shell.execute_reply":"2025-04-03T06:55:19.623858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Define the Classifier","metadata":{"id":"W4p4tYAraGVn"}},{"cell_type":"code","source":"num_classes = 3  # Adjust this number based on your dataset.\nclassifier = nn.Linear(768, num_classes).cuda()\n","metadata":{"id":"rGFXEjdsaOW1","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.624833Z","iopub.status.idle":"2025-04-03T06:55:19.625195Z","shell.execute_reply":"2025-04-03T06:55:19.625037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Set Up Optimizer and Loss Criterion","metadata":{"id":"X0ZKbi8UaQig"}},{"cell_type":"code","source":"clf_optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\ncriterion_cls = nn.CrossEntropyLoss()\n","metadata":{"id":"R19laeJNaTp9","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.625921Z","iopub.status.idle":"2025-04-03T06:55:19.626280Z","shell.execute_reply":"2025-04-03T06:55:19.626120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Training Loop for the Classifier (Using Training Data Only)","metadata":{"id":"1L2khmV-aWGm"}},{"cell_type":"code","source":"num_epochs_clf = 5\nbest_train_acc = 0.0  # Best training accuracy so far.\n\nfor epoch in range(num_epochs_clf):\n    epoch_start_time = time.time()\n    classifier.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for context_block, _, label in train_loader:\n        context_block = context_block.cuda()  # [B, C, 224, 224]\n        label = label.cuda()\n\n        with torch.no_grad():\n            features = context_encoder(context_block)  # [B, 768]\n\n        logits = classifier(features)  # [B, num_classes]\n        loss = criterion_cls(logits, label)\n\n        clf_optimizer.zero_grad()\n        loss.backward()\n        clf_optimizer.step()\n\n        running_loss += loss.item() * context_block.size(0)\n        preds = logits.argmax(dim=1)\n        correct_train += (preds == label).sum().item()\n        total_train += label.size(0)\n\n    epoch_train_loss = running_loss / len(train_ijepa_dataset)\n    epoch_train_acc = correct_train / total_train\n    epoch_time = time.time() - epoch_start_time\n\n    print(f\"Epoch {epoch+1}/{num_epochs_clf} - Train Loss: {epoch_train_loss:.10f} | Train Acc: {epoch_train_acc*100:.10f}% | Time: {epoch_time:.2f}s\")\n\n    # Save checkpoint if training accuracy improves.\n    if epoch_train_acc > best_train_acc:\n        best_train_acc = epoch_train_acc\n        checkpoint = {\n            'epoch': epoch+1,\n            'classifier_state_dict': classifier.state_dict(),\n            'optimizer_state_dict': clf_optimizer.state_dict(),\n            'train_loss': epoch_train_loss,\n            'train_acc': epoch_train_acc\n        }\n        torch.save(checkpoint, os.path.join(base_dir,\"ijepa_classifier_best.pth\"))\n        print(f\"Checkpoint saved at epoch {epoch+1} with Train Acc: {epoch_train_acc*100:.10f}%\")\n\nprint(\"Classifier training complete!\")","metadata":{"id":"3nQoDWvwaZ4g","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.627081Z","iopub.status.idle":"2025-04-03T06:55:19.627443Z","shell.execute_reply":"2025-04-03T06:55:19.627282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import output\noutput.eval_js('google.colab.kernel.disconnect()')","metadata":{"id":"OfvaWDggmMKX","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.628157Z","iopub.status.idle":"2025-04-03T06:55:19.628481Z","shell.execute_reply":"2025-04-03T06:55:19.628361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kill -9 -1","metadata":{"id":"TDqi8kpNuRWV","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:55:19.629037Z","iopub.status.idle":"2025-04-03T06:55:19.629352Z","shell.execute_reply":"2025-04-03T06:55:19.629242Z"}},"outputs":[],"execution_count":null}]}