{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11224133,
          "sourceType": "datasetVersion",
          "datasetId": 7010008
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raihanewubd/selfSupervised/blob/main/i_jepa_aav_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "\n",
        "\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "ab0f765f-50d0-4e71-934b-69036f927618",
        "_cell_guid": "1b74e769-17ce-492e-bbf7-eb90938bae5d",
        "id": "7Qm5o5BpuTyt",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "_uuid": "b02bcb6e-1d67-4b0c-9d11-a22f3ab07a61",
        "_cell_guid": "d0758325-ed5a-4988-b20a-51539395a900",
        "trusted": true,
        "id": "Gxnc6znruQbZ",
        "outputId": "9a7e3c98-b215-41a6-da85-f0ccbf58bc80",
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base directory and file name for saving the classifier checkpoint.\n",
        "base_dir = \"/kaggle/working/AAVDATASET/spectrogram\"\n",
        "base_dir = \"/content/drive/MyDrive/AAVDATASET/spectrogram\""
      ],
      "metadata": {
        "id": "51MrUfzzXm_a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = base_dir\n",
        "data_dir = os.path.join(base_dir,'dataset')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "_uuid": "2bb0d814-c433-4069-b2de-b8afb06f8c33",
        "_cell_guid": "cfbd3054-08ff-4e37-a1dd-a9816ded260d",
        "trusted": true,
        "id": "fKthbkn-t67J",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device for GPU acceleration if available.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zIe8gpxxU60",
        "outputId": "2047897e-2614-44a2-e53b-a29879c16e7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_blocks(image, context_scale=0.85, target_scale=0.2, num_targets=4, max_overlap=0.5):\n",
        "    # Extract a central context block.\n",
        "    _, H, W = image.shape\n",
        "    context_size = int(context_scale * H)\n",
        "    top = (H - context_size) // 2\n",
        "    left = (W - context_size) // 2\n",
        "    context_block = image[:, top:top+context_size, left:left+context_size]\n",
        "    context_block = torch.nn.functional.interpolate(\n",
        "        context_block.unsqueeze(0),\n",
        "        size=(224, 224),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    ).squeeze(0)\n",
        "\n",
        "    # Extract num_targets target blocks randomly.\n",
        "    target_blocks = []\n",
        "    for _ in range(num_targets):\n",
        "        target_size = int(target_scale * H)\n",
        "        top_t = random.randint(0, H - target_size)\n",
        "        left_t = random.randint(0, W - target_size)\n",
        "        target_block = image[:, top_t:top_t+target_size, left_t:left_t+target_size]\n",
        "        target_block = torch.nn.functional.interpolate(\n",
        "            target_block.unsqueeze(0),\n",
        "            size=(224, 224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze(0)\n",
        "        target_blocks.append(target_block)\n",
        "    target_blocks = torch.stack(target_blocks)\n",
        "    return context_block, target_blocks, (top, left, context_size), None\n",
        "\n"
      ],
      "metadata": {
        "id": "cuurpOHkxbxk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sample(sample, context_scale, target_scale, num_targets):\n",
        "    # Unpack sample: sample is ((img, label), image_path)\n",
        "    (img, label), image_path = sample\n",
        "    # Move image to GPU if available.\n",
        "    img = img.to(device)\n",
        "    context_block, target_blocks, _, _ = extract_blocks(img, context_scale, target_scale, num_targets)\n",
        "    # Bring results back to CPU before caching.\n",
        "    return (context_block.cpu(), target_blocks.cpu(), label, image_path)"
      ],
      "metadata": {
        "id": "oj2gcdgq4hvd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrecomputedIJEPADataset(Dataset):\n",
        "    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4, cache_file=None):\n",
        "        self.cache_file = cache_file\n",
        "        if cache_file and os.path.exists(cache_file):\n",
        "            # Load precomputed data from disk.\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "        else:\n",
        "            # Create a list of samples along with their original image paths (if available) using a progress bar.\n",
        "            if hasattr(base_dataset, 'samples'):\n",
        "                base_samples = [\n",
        "                    (base_dataset[i], base_dataset.samples[i][0])\n",
        "                    for i in tqdm(range(len(base_dataset)), desc=\"Loading samples\")\n",
        "                ]\n",
        "            else:\n",
        "                base_samples = [\n",
        "                    (sample, None) for sample in tqdm(base_dataset, desc=\"Loading samples\")\n",
        "                ]\n",
        "\n",
        "            # Process samples sequentially with a progress bar.\n",
        "            self.data = []\n",
        "            for sample in tqdm(base_samples, desc=\"Processing samples\"):\n",
        "                result = process_sample(sample, context_scale, target_scale, num_targets)\n",
        "                self.data.append(result)\n",
        "\n",
        "            if cache_file:\n",
        "                with open(cache_file, 'wb') as f:\n",
        "                    pickle.dump(self.data, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "metadata": {
        "id": "w3876qql71lv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Timing the loading of the dataset and DataLoader\n",
        "cache_path = os.path.join(base_dir,\"precomputed_fulldataset_aav.pkl\")\n",
        "print(cache_path)\n",
        "start_time = time.time()\n",
        "dataset_aav_ijepa = PrecomputedIJEPADataset(dataset, cache_file=cache_path)\n",
        "end_time_train_ijepa_dataset = time.time()\n",
        "dataloader_aav_ijepa = DataLoader(dataset_aav_ijepa, batch_size=32, shuffle=True)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken to load dataset: {end_time_train_ijepa_dataset - start_time:.4f} seconds and DataLoader: {end_time - end_time_train_ijepa_dataset:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54OBcd7SgFde",
        "outputId": "bfa2b185-0136-43ab-a541-410d9f5ae95d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AAVDATASET/spectrogram/precomputed_fulldataset_aav.pkl\n",
            "Time taken to load dataset: 177.8171 seconds and DataLoader: 0.0003 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(dataset_aav_ijepa)\n",
        "print(f\"Number of images in the dataset: {num_images}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEaXCJIKJLBt",
        "outputId": "9a487a6b-18c4-4478-bc20-da5f7efff64a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the dataset: 3513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_batches = len(dataloader_aav_ijepa)\n",
        "print(\"Total number of batches:\", total_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pXBRPzuZPZH",
        "outputId": "bd63ba50-dbe3-4e88-f815-fe4587a2d7f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of batches: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Define the split ratio (e.g., 80% train, 20% test)\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1 - train_ratio\n",
        "\n",
        "# Get the total number of samples in the dataset.\n",
        "num_samples = len(dataset_aav_ijepa)\n",
        "\n",
        "# Create a list of indices for all samples in the dataset.\n",
        "indices = list(range(num_samples))\n",
        "\n",
        "# Split the indices into train and test sets using train_test_split.\n",
        "train_indices, test_indices = train_test_split(indices, test_size=test_ratio, random_state=42)  # Set random_state for reproducibility.\n",
        "\n",
        "# Create Subset datasets for train and test using the split indices.\n",
        "train_dataset_aav_ijepa = Subset(dataset_aav_ijepa, train_indices)\n",
        "test_dataset_aav_ijepa = Subset(dataset_aav_ijepa, test_indices)\n",
        "\n",
        "# Create DataLoaders for the train and test datasets.\n",
        "train_loader_aav_ijepa = torch.utils.data.DataLoader(train_dataset_aav_ijepa, batch_size=32, shuffle=True)\n",
        "test_loader_aav_ijepa = torch.utils.data.DataLoader(test_dataset_aav_ijepa, batch_size=32, shuffle=False)  # No need to shuffle the test set.\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset_aav_ijepa)}\")\n",
        "print(f\"Testing set size: {len(test_dataset_aav_ijepa)}\")'''"
      ],
      "metadata": {
        "id": "EqNu-u2qLX44",
        "outputId": "24f917f4-1e58-4f72-d50c-7ff688eac560",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2810\n",
            "Testing set size: 703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Subset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the split ratios (e.g., 70% train, 15% validation, 15% test)\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Get the total number of samples in the dataset.\n",
        "num_samples = len(dataset_aav_ijepa)\n",
        "\n",
        "# 1. Split into train and (validation + test)\n",
        "train_indices, val_test_indices = train_test_split(\n",
        "    list(range(num_samples)),\n",
        "    test_size=val_ratio + test_ratio,\n",
        "    random_state=42  # Set random_state for reproducibility\n",
        ")\n",
        "\n",
        "# 2. Split (validation + test) into validation and test\n",
        "val_indices, test_indices = train_test_split(\n",
        "    val_test_indices,\n",
        "    test_size=test_ratio / (val_ratio + test_ratio),\n",
        "    random_state=42  # Set random_state for reproducibility\n",
        ")\n",
        "\n",
        "# Create Subset datasets for train, validation, and test\n",
        "train_dataset_aav_ijepa = Subset(dataset_aav_ijepa, train_indices)\n",
        "val_dataset_aav_ijepa = Subset(dataset_aav_ijepa, val_indices)\n",
        "test_dataset_aav_ijepa = Subset(dataset_aav_ijepa, test_indices)\n",
        "\n",
        "# Create DataLoaders for train, validation, and test\n",
        "train_loader_aav_ijepa = torch.utils.data.DataLoader(train_dataset_aav_ijepa, batch_size=32, shuffle=True)\n",
        "val_loader_aav_ijepa = torch.utils.data.DataLoader(val_dataset_aav_ijepa, batch_size=32, shuffle=False)\n",
        "test_loader_aav_ijepa = torch.utils.data.DataLoader(test_dataset_aav_ijepa, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset_aav_ijepa)}\")\n",
        "print(f\"Validation set size: {len(val_dataset_aav_ijepa)}\")\n",
        "print(f\"Testing set size: {len(test_dataset_aav_ijepa)}\")"
      ],
      "metadata": {
        "id": "YcNSpd5mLi3g",
        "outputId": "3ff04da4-5c3f-40f6-e0c7-fe0794fa41cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2459\n",
            "Validation set size: 527\n",
            "Testing set size: 527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vit_encoder():\n",
        "    model = vit_b_16(pretrained=False)\n",
        "    model.heads = nn.Identity()  # remove classification head\n",
        "    return model"
      ],
      "metadata": {
        "_uuid": "f3ad3daa-80ba-4ae8-8b4e-770815bad02e",
        "_cell_guid": "7aeb9d44-2fd8-4da1-87c0-4b992adb53db",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "YQIK4mND8qK1"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context_encoder = get_vit_encoder().cuda()\n",
        "target_encoder  = get_vit_encoder().cuda()\n",
        "target_encoder.load_state_dict(context_encoder.state_dict())"
      ],
      "metadata": {
        "_uuid": "b79e21b1-3b97-4dfc-958f-bddb3c94c746",
        "_cell_guid": "2d8198b7-dbf5-4228-975a-8c733cea1a07",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "rkq_fT7g8qK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937b6613-1e96-4188-9910-59f4a58f9004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=768, output_dim=768, num_targets=4):\n",
        "        super().__init__()\n",
        "        self.num_targets = num_targets\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim * num_targets)\n",
        "        )\n",
        "    def forward(self, context_repr):\n",
        "        pred = self.mlp(context_repr)\n",
        "        # Reshape to [B, num_targets, output_dim]\n",
        "        return pred.view(-1, self.num_targets, pred.size(-1) // self.num_targets)"
      ],
      "metadata": {
        "_uuid": "e303f4ac-a09b-4f56-b997-6b364d63a675",
        "_cell_guid": "9206cc42-0845-4604-ab30-6852e28e2dcb",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "OApD7Fy68qK1"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Set up optimizer, loss, and EMA update (same as your CIFAR code).\n",
        "predictor = Predictor().cuda()\n",
        "optimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=1e-1)\n",
        "criterion = nn.MSELoss()\n",
        "ema_decay = 0.99"
      ],
      "metadata": {
        "_uuid": "66d2337f-fb52-4652-9da7-242ce9d9d5e1",
        "_cell_guid": "e10ef302-1659-4a97-9bbb-2de90ac902c9",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZsZNIdEk8qK2"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def update_ema(model, model_ema, beta):\n",
        "    for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n",
        "        param_ema.data.mul_(beta).add_(param.data, alpha=1 - beta)"
      ],
      "metadata": {
        "_uuid": "0e90599a-d03d-4231-a58a-1272a3613cc3",
        "_cell_guid": "aece458a-d1da-4334-804f-b054cf8ed0fc",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "kM60lfOw8qK2"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for visualizations if it doesn't exist.\n",
        "#viz_dir = \"/kaggle/working/viz\"\n",
        "#os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "num_epochs = 10\n",
        "ema_decay = 0.1\n",
        "best_loss = float('inf')\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    context_encoder.train()\n",
        "    predictor.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Enumerate over batches with a progress bar.\n",
        "    for batch_idx, (context_block, target_blocks, _) in enumerate(tqdm(train_dataset_aav_ijepa, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "        context_block = context_block.cuda()            # [B, C, 224, 224]\n",
        "        target_blocks = target_blocks.cuda()              # [B, num_targets, C, 224, 224]\n",
        "\n",
        "        # Forward pass through context encoder and predictor.\n",
        "        context_repr = context_encoder(context_block)     # [B, 768]\n",
        "        preds = predictor(context_repr)                   # [B, num_targets, 768]\n",
        "\n",
        "        B, num_targets, C, Ht, Wt = target_blocks.shape\n",
        "        target_blocks_flat = target_blocks.view(B * num_targets, C, Ht, Wt)\n",
        "        with torch.no_grad():\n",
        "            target_repr_flat = target_encoder(target_blocks_flat)\n",
        "        target_repr = target_repr_flat.view(B, num_targets, -1)\n",
        "\n",
        "        loss = criterion(preds, target_repr)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        update_ema(context_encoder, target_encoder, ema_decay)\n",
        "        running_loss += loss.item() * context_block.size(0)\n",
        "\n",
        "        # --- Visualization for first image of the current batch ---\n",
        "        '''with torch.no_grad():\n",
        "            # Get the first sample's context block and compute its feature vector.\n",
        "            context_img = context_block[0].cpu()  # shape: [C, 224, 224]\n",
        "            context_feat = context_encoder(context_block[0].unsqueeze(0)).cpu().squeeze(0)  # shape: [768]\n",
        "            # Reshape feature vector to a 2D heatmap (24x32).\n",
        "            context_heat = context_feat.view(24, 32).numpy()\n",
        "\n",
        "            # For target, choose the first target block of the first sample.\n",
        "            target_img = target_blocks[0][0].cpu()  # shape: [C, 224, 224]\n",
        "            target_feat = target_encoder(target_blocks[0][0].unsqueeze(0).to(context_block.device)).cpu().squeeze(0)\n",
        "            target_heat = target_feat.view(24, 32).numpy()\n",
        "\n",
        "            # Plot the images and corresponding heatmaps.\n",
        "            fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "            # Display context block image.\n",
        "            if context_img.shape[0] == 1:\n",
        "                axs[0, 0].imshow(context_img.squeeze(), cmap='gray')\n",
        "            else:\n",
        "                axs[0, 0].imshow(context_img.permute(1, 2, 0))\n",
        "            axs[0, 0].set_title(\"Context Block\")\n",
        "            axs[0, 0].axis(\"off\")\n",
        "\n",
        "            # Display context feature heatmap.\n",
        "            im0 = axs[0, 1].imshow(context_heat, cmap=\"viridis\")\n",
        "            axs[0, 1].set_title(\"Context Feature Heatmap\")\n",
        "            axs[0, 1].axis(\"off\")\n",
        "            fig.colorbar(im0, ax=axs[0, 1])\n",
        "\n",
        "            # Display target block image.\n",
        "            if target_img.shape[0] == 1:\n",
        "                axs[1, 0].imshow(target_img.squeeze(), cmap='gray')\n",
        "            else:\n",
        "                axs[1, 0].imshow(target_img.permute(1, 2, 0))\n",
        "            axs[1, 0].set_title(\"Target Block\")\n",
        "            axs[1, 0].axis(\"off\")\n",
        "\n",
        "            # Display target feature heatmap.\n",
        "            im1 = axs[1, 1].imshow(target_heat, cmap=\"viridis\")\n",
        "            axs[1, 1].set_title(\"Target Feature Heatmap\")\n",
        "            axs[1, 1].axis(\"off\")\n",
        "            fig.colorbar(im1, ax=axs[1, 1])\n",
        "\n",
        "            # Save the visualization figure with epoch and batch number.\n",
        "            viz_path = os.path.join(viz_dir, f\"epoch{epoch+1}_batch{batch_idx+1}.png\")\n",
        "            plt.savefig(viz_path)\n",
        "            plt.close(fig)'''\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset_aav_ijepa)\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.10f} - Epoch Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save checkpoint if current epoch loss is lower than previous best.\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'context_encoder_state_dict': context_encoder.state_dict(),\n",
        "            'target_encoder_state_dict': target_encoder.state_dict(),\n",
        "            'predictor_state_dict': predictor.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(base_dir,\"ijepa_checkpoint_best.pth\"))\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "total_train_time = time.time() - total_start_time\n",
        "print(f\"Total Training Time: {total_train_time:.2f}s\")"
      ],
      "metadata": {
        "_uuid": "c8659b2c-5286-4027-b97b-c1ac887bb5f6",
        "_cell_guid": "0182b713-044f-4839-9cfc-0369f1eec96d",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IjnBW1lE8qK2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Classifier"
      ],
      "metadata": {
        "id": "clZWxsbpZ9EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the Saved Checkpoint for the Self-Supervised Model"
      ],
      "metadata": {
        "id": "GQnzVW9aaDL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(os.path.join(base_dir, \"ijepa_checkpoint_best.pth\"))\n",
        "context_encoder.load_state_dict(checkpoint['context_encoder_state_dict'])\n",
        "# Freeze the context encoder.\n",
        "context_encoder.eval()\n",
        "for param in context_encoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "TXG_-3UPaHTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the Classifier"
      ],
      "metadata": {
        "id": "W4p4tYAraGVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3  # Adjust this number based on your dataset.\n",
        "classifier = nn.Linear(768, num_classes).cuda()\n"
      ],
      "metadata": {
        "id": "rGFXEjdsaOW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Set Up Optimizer and Loss Criterion"
      ],
      "metadata": {
        "id": "X0ZKbi8UaQig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
        "criterion_cls = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "R19laeJNaTp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Loop for the Classifier (Using Training Data Only)"
      ],
      "metadata": {
        "id": "1L2khmV-aWGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs_clf = 10\n",
        "best_train_acc = 0.0  # Best training accuracy so far.\n",
        "\n",
        "for epoch in range(num_epochs_clf):\n",
        "    epoch_start_time = time.time()\n",
        "    classifier.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for context_block, _, label in train_loader:\n",
        "        context_block = context_block.cuda()  # [B, C, 224, 224]\n",
        "        label = label.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = context_encoder(context_block)  # [B, 768]\n",
        "\n",
        "        logits = classifier(features)  # [B, num_classes]\n",
        "        loss = criterion_cls(logits, label)\n",
        "\n",
        "        clf_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clf_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * context_block.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_train += (preds == label).sum().item()\n",
        "        total_train += label.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_ijepa_dataset)\n",
        "    epoch_train_acc = correct_train / total_train\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_clf} - Train Loss: {epoch_train_loss:.10f} | Train Acc: {epoch_train_acc*100:.10f}% | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save checkpoint if training accuracy improves.\n",
        "    if epoch_train_acc > best_train_acc:\n",
        "        best_train_acc = epoch_train_acc\n",
        "        checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'classifier_state_dict': classifier.state_dict(),\n",
        "            'optimizer_state_dict': clf_optimizer.state_dict(),\n",
        "            'train_loss': epoch_train_loss,\n",
        "            'train_acc': epoch_train_acc\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(base_dir,\"ijepa_classifier_best.pth\"))\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1} with Train Acc: {epoch_train_acc*100:.10f}%\")\n",
        "\n",
        "print(\"Classifier training complete!\")"
      ],
      "metadata": {
        "id": "3nQoDWvwaZ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.eval_js('google.colab.kernel.disconnect()')"
      ],
      "metadata": {
        "id": "OfvaWDggmMKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "metadata": {
        "id": "TDqi8kpNuRWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}