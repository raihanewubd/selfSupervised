{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "name": "i_jepa_aav.ipynb",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11224133,
          "sourceType": "datasetVersion",
          "datasetId": 7010008
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raihanewubd/selfSupervised/blob/main/i_jepa_aav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "ECaBLnsn8qKX"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "beautifulminnd_aav_spectrogram_path = kagglehub.dataset_download('beautifulminnd/aav-spectrogram')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "qlsmrDvX8qKm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from torchvision.models import vit_b_16\n",
        "from collections import Counter\n",
        "from tqdm import tqdm # for progress bar\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "_uuid": "ab0f765f-50d0-4e71-934b-69036f927618",
        "_cell_guid": "1b74e769-17ce-492e-bbf7-eb90938bae5d",
        "id": "7Qm5o5BpuTyt",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Number of CPU cores:\", os.cpu_count())\n"
      ],
      "metadata": {
        "id": "c78ELuxxevt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "print(\"Number of CPU cores:\", multiprocessing.cpu_count())\n"
      ],
      "metadata": {
        "id": "Y67lvNw1exxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "_uuid": "b02bcb6e-1d67-4b0c-9d11-a22f3ab07a61",
        "_cell_guid": "d0758325-ed5a-4988-b20a-51539395a900",
        "trusted": true,
        "id": "Gxnc6znruQbZ",
        "outputId": "038864c0-7489-4d9b-f757-db88fea7e0c5",
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base directory and file name for saving the classifier checkpoint.\n",
        "base_dir = \"/kaggle/working/AAVDATASET/spectrogram\"\n",
        "base_dir = '/content/drive/MyDrive/AAVDATASET/spectrogram'"
      ],
      "metadata": {
        "id": "51MrUfzzXm_a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/kaggle/input/aav-spectrogram/spectrogram'\n",
        "data_dir = '/content/drive/MyDrive/AAVDATASET/spectrogram/dataset'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "_uuid": "2bb0d814-c433-4069-b2de-b8afb06f8c33",
        "_cell_guid": "cfbd3054-08ff-4e37-a1dd-a9816ded260d",
        "trusted": true,
        "id": "fKthbkn-t67J",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device for GPU acceleration if available.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_blocks(image, context_scale=0.85, target_scale=0.2, num_targets=4, max_overlap=0.5):\n",
        "    # Extract a central context block.\n",
        "    _, H, W = image.shape\n",
        "    context_size = int(context_scale * H)\n",
        "    top = (H - context_size) // 2\n",
        "    left = (W - context_size) // 2\n",
        "    context_block = image[:, top:top+context_size, left:left+context_size]\n",
        "    context_block = torch.nn.functional.interpolate(\n",
        "        context_block.unsqueeze(0),\n",
        "        size=(224, 224),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    ).squeeze(0)\n",
        "\n",
        "    # Extract num_targets target blocks randomly.\n",
        "    target_blocks = []\n",
        "    for _ in range(num_targets):\n",
        "        target_size = int(target_scale * H)\n",
        "        top_t = random.randint(0, H - target_size)\n",
        "        left_t = random.randint(0, W - target_size)\n",
        "        target_block = image[:, top_t:top_t+target_size, left_t:left_t+target_size]\n",
        "        target_block = torch.nn.functional.interpolate(\n",
        "            target_block.unsqueeze(0),\n",
        "            size=(224, 224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze(0)\n",
        "        target_blocks.append(target_block)\n",
        "    target_blocks = torch.stack(target_blocks)\n",
        "    return context_block, target_blocks, (top, left, context_size), None\n",
        "\n",
        "def process_sample(sample, context_scale, target_scale, num_targets):\n",
        "    # Unpack sample: sample is ((img, label), image_path)\n",
        "    (img, label), image_path = sample\n",
        "    # Move image to GPU if available.\n",
        "    img = img.to(device)\n",
        "    context_block, target_blocks, _, _ = extract_blocks(img, context_scale, target_scale, num_targets)\n",
        "    # Bring results back to CPU before caching.\n",
        "    return (context_block.cpu(), target_blocks.cpu(), label, image_path)\n",
        "\n",
        "class PrecomputedIJEPADataset(Dataset):\n",
        "    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4, cache_file=None):\n",
        "        self.cache_file = cache_file\n",
        "        if cache_file and os.path.exists(cache_file):\n",
        "            # Load precomputed data from disk.\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "        else:\n",
        "            # Create a list of samples along with their original image paths (if available) using a progress bar.\n",
        "            if hasattr(base_dataset, 'samples'):\n",
        "                base_samples = [\n",
        "                    (base_dataset[i], base_dataset.samples[i][0])\n",
        "                    for i in tqdm(range(len(base_dataset)), desc=\"Loading samples\")\n",
        "                ]\n",
        "            else:\n",
        "                base_samples = [\n",
        "                    (sample, None) for sample in tqdm(base_dataset, desc=\"Loading samples\")\n",
        "                ]\n",
        "\n",
        "            # Process samples sequentially with a progress bar.\n",
        "            self.data = []\n",
        "            for sample in tqdm(base_samples, desc=\"Processing samples\"):\n",
        "                result = process_sample(sample, context_scale, target_scale, num_targets)\n",
        "                self.data.append(result)\n",
        "\n",
        "            if cache_file:\n",
        "                with open(cache_file, 'wb') as f:\n",
        "                    pickle.dump(self.data, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "metadata": {
        "id": "w3876qql71lv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Timing the loading of the dataset and DataLoader\n",
        "cache_path = os.path.join(base_dir,\"precomputed_fulldataset_aav.pkl\")\n",
        "print(cache_path)\n",
        "start_time = time.time()\n",
        "dataset_aav_ijepa = PrecomputedIJEPADataset(dataset, cache_file=cache_path)\n",
        "end_time_train_ijepa_dataset = time.time()\n",
        "dataloader_aav_ijepa = DataLoader(dataset_aav_ijepa, batch_size=32, shuffle=True)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken to load dataset: {end_time_train_ijepa_dataset - start_time:.4f} seconds and DataLoader: {end_time - start_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54OBcd7SgFde",
        "outputId": "b08e434b-ffda-4a03-c9f9-e2c274f84a69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AAVDATASET/spectrogram/precomputed_fulldataset_aav.pkl\n",
            "Time taken to load dataset: 71.7497 seconds and DataLoader: 71.7502 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_batches = len(dataloader_aav_ijepa)\n",
        "print(\"Total number of batches:\", total_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pXBRPzuZPZH",
        "outputId": "694ad32a-27c6-4e5f-a2b1-f483958f5827"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of batches: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(dataset_aav_ijepa)\n",
        "print(f\"Number of images in the dataset: {num_images}\")"
      ],
      "metadata": {
        "id": "KEaXCJIKJLBt",
        "outputId": "6fd2622d-fe59-4c94-bde9-8939225c49d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the dataset: 3513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the labels from the dataset:\n",
        "labels = [sample[2] for sample in dataset_aav_ijepa.data]\n",
        "\n",
        "# Count the occurrences of each label using Counter:\n",
        "class_counts = Counter(labels)\n",
        "\n",
        "# Get the class labels and their corresponding counts:\n",
        "class_labels, counts = zip(*class_counts.items())\n",
        "\n",
        "# Create the bar graph:\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(class_labels, counts)\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Number of Images per Class\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add text labels above the bars:\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(class_labels[i], count + 5,  # Adjust vertical position as needed\n",
        "             str(count), ha='center', va='bottom')\n",
        "\n",
        "# Add text labels for class names below the x-axis:\n",
        "for i, label in enumerate(class_labels):\n",
        "    plt.text(label, -10,  # Adjust vertical position as needed\n",
        "             dataset.classes[label], ha='center', va='top', rotation=45)  # Assuming dataset has 'classes' attribute\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_k0oMvPiJZBF",
        "outputId": "495cca0d-bc9d-4032-f6c7-ec728c9ab054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Limit to a few batches for clarity (e.g., first 5 batches)\n",
        "num_batches_to_plot = 100\n",
        "batch_dists = []  # list to store counts per batch\n",
        "batch_labels = [] # list to store batch labels\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    if batch_idx >= num_batches_to_plot:\n",
        "        break\n",
        "    # Count labels in the current batch\n",
        "    batch_counts = Counter(labels.numpy())\n",
        "    # Ensure counts are in order of class indices; assumes dataset.classes exist\n",
        "    counts = [batch_counts.get(i, 0) for i in range(len(dataset.classes))]\n",
        "    batch_dists.append(counts)\n",
        "    batch_labels.append(f'Batch {batch_idx}')\n",
        "\n",
        "# Create subplots: one row per batch\n",
        "fig, axes = plt.subplots(nrows=num_batches_to_plot, ncols=1, figsize=(6, num_batches_to_plot * 3))\n",
        "\n",
        "# Handle the case when there's only one subplot (axes is not a list)\n",
        "if num_batches_to_plot == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Plot a bar chart for each batch's class distribution\n",
        "for ax, counts, label in zip(axes, batch_dists, batch_labels):\n",
        "    ax.bar(range(len(counts)), counts)\n",
        "    ax.set_xticks(range(len(counts)))\n",
        "    ax.set_xticklabels(dataset.classes, rotation=45)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(label)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kyw7ucooKGy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each sample in dataset.imgs is a tuple (image_path, class_index)\n",
        "#class_counts = Counter(label for _, label in dataset_aav_ijepa.imgs)\n",
        "#print(\"Total images per class:\", class_counts)\n",
        "\n",
        "# Optionally, map indices back to class names using dataset.class_to_idx\n",
        "idx_to_class = {v: k for k, v in dataset_aav_ijepa.class_to_idx.items()}\n",
        "for idx, count in class_counts.items():\n",
        "    print(f\"Class '{idx_to_class[idx]}': {count} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sxi1FoatY1m-",
        "outputId": "863d545c-9a0a-40b8-85a3-fe58491eff16"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PrecomputedIJEPADataset' object has no attribute 'class_to_idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e1190dabb739>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Optionally, map indices back to class names using dataset.class_to_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0midx_to_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_aav_ijepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Class '{idx_to_class[idx]}': {count} images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PrecomputedIJEPADataset' object has no attribute 'class_to_idx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels,_,_ in dataloader_aav_ijepa:\n",
        "    # Convert the labels tensor to a list of integers\n",
        "    labels_list = labels.cpu().numpy().tolist()\n",
        "    # Flatten the list (if necessary)\n",
        "    labels_flat = [item for sublist in labels_list for item in sublist]\n",
        "    # Count occurrences using Counter\n",
        "    batch_counts = Counter(labels_flat)\n",
        "    print(\"Images per class in this batch:\", batch_counts)\n",
        "    break  # Remove break to inspect more batches"
      ],
      "metadata": {
        "id": "fCjlDzprI7mV",
        "outputId": "b1d170bd-67d6-46ca-faf6-4cba2916ba53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-401cb5dc8125>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Count occurrences using Counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbatch_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Images per class in this batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Remove break to inspect more batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         '''\n\u001b[1;32m    598\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels,_,_ in dataloader_aav_ijepa:\n",
        "    batch_counts = Counter(labels.tolist())\n",
        "    print(\"Images per class in this batch:\", batch_counts)\n",
        "    break  # Remove break to inspect more batches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "WVM17LOzZBG3",
        "outputId": "cc1a1c56-8d5f-485d-b333-2d92e77b8d33"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-abda322764ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader_aav_ijepa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Images per class in this batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Remove break to inspect more batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         '''\n\u001b[1;32m    598\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Limit to a few batches for clarity (e.g., first 5 batches)\n",
        "num_batches_to_plot = 5\n",
        "batch_dists = []  # list to store counts per batch\n",
        "batch_labels = [] # list to store batch labels\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    if batch_idx >= num_batches_to_plot:\n",
        "        break\n",
        "    # Count labels in the current batch\n",
        "    batch_counts = Counter(labels.numpy())\n",
        "    # Ensure counts are in order of class indices; assumes dataset.classes exist\n",
        "    counts = [batch_counts.get(i, 0) for i in range(len(dataset.classes))]\n",
        "    batch_dists.append(counts)\n",
        "    batch_labels.append(f'Batch {batch_idx}')\n",
        "\n",
        "# Create subplots: one row per batch\n",
        "fig, axes = plt.subplots(nrows=num_batches_to_plot, ncols=1, figsize=(6, num_batches_to_plot * 3))\n",
        "\n",
        "# Handle the case when there's only one subplot (axes is not a list)\n",
        "if num_batches_to_plot == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Plot a bar chart for each batch's class distribution\n",
        "for ax, counts, label in zip(axes, batch_dists, batch_labels):\n",
        "    ax.bar(range(len(counts)), counts)\n",
        "    ax.set_xticks(range(len(counts)))\n",
        "    ax.set_xticklabels(dataset.classes, rotation=45)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(label)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v_zDv05YZwwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test loading\n",
        "images, labels = next(iter(dataloader))\n",
        "print(\"Batch shape:\", images.shape)  # Should be [B, C, 224, 224]"
      ],
      "metadata": {
        "_uuid": "5d688a1b-2546-4240-bbb7-5aec8d108cfc",
        "_cell_guid": "a8f3ba24-1e87-429a-bbee-4909d3832d58",
        "trusted": true,
        "id": "C9kEg-mIKiF3",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "_uuid": "ebcd8a59-646f-41d1-98a2-6111730750c6",
        "_cell_guid": "577f43b2-192c-4d42-9928-3612f02483bc",
        "trusted": true,
        "id": "N2As1-TPux9B",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(6):\n",
        "    img = images[i].permute(1, 2, 0) * 0.5 + 0.5  # Unnormalize\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(img.squeeze(), cmap='viridis')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Sample Spectrograms (Resized to 224224)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "_uuid": "7b5e6218-b07f-4701-a372-28b857d114d6",
        "_cell_guid": "551e8024-3177-4665-896d-27a3497990e0",
        "trusted": true,
        "id": "rRDzaHnJKw9k",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def count_images_per_class(train_loader):\n",
        "    \"\"\"Counts the number of images per class in a train_loader.\n",
        "\n",
        "    Args:\n",
        "        train_loader: The DataLoader for the training dataset.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are class labels and values are the corresponding counts.\n",
        "    \"\"\"\n",
        "\n",
        "    all_labels = []\n",
        "    for images, labels in tqdm(train_loader, desc=\"Counting images per class\"):\n",
        "        all_labels.extend(labels.tolist())  # Extend the list with labels from the batch\n",
        "\n",
        "    class_counts = Counter(all_labels)  # Count occurrences of each label\n",
        "\n",
        "    return class_counts"
      ],
      "metadata": {
        "_uuid": "c9b49613-e5e4-4a6f-ae14-7287ae13e3a4",
        "_cell_guid": "68672c2b-407b-42e6-a744-2523bf327831",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ktzymg7h8qKy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the counts and print them\n",
        "class_counts = count_images_per_class(train_loader)\n",
        "\n",
        "print(\"Images per class in train_loader:\")\n",
        "for class_label, count in class_counts.items():\n",
        "    print(f\"Class {class_label}: {count} images\")"
      ],
      "metadata": {
        "_uuid": "3e31c353-e70e-4e37-a58f-1a4d5ffab8d1",
        "_cell_guid": "46c3eb6b-6513-46a0-a732-946df9564d7e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1X3l3kJ08qKy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the counts and print them\n",
        "class_counts = count_images_per_class(val_loader)\n",
        "\n",
        "print(\"Images per class in train_loader:\")\n",
        "for class_label, count in class_counts.items():\n",
        "    print(f\"Class {class_label}: {count} images\")"
      ],
      "metadata": {
        "_uuid": "f66b9f67-0023-42ea-bea2-456f7146ab2e",
        "_cell_guid": "a8f02f58-fcb6-4234-9407-45f1c6e3e11e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "t4ieAvgL8qKz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_blocks(image, context_scale=0.85, target_scale=0.2, num_targets=4, max_overlap=0.5):\n",
        "    # Extract a central context block.\n",
        "    _, H, W = image.shape\n",
        "    context_size = int(context_scale * H)\n",
        "    top = (H - context_size) // 2\n",
        "    left = (W - context_size) // 2\n",
        "    context_block = image[:, top:top+context_size, left:left+context_size]\n",
        "    context_block = torch.nn.functional.interpolate(context_block.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
        "\n",
        "    # Extract num_targets target blocks randomly.\n",
        "    target_blocks = []\n",
        "    for _ in range(num_targets):\n",
        "        target_size = int(target_scale * H)\n",
        "        top_t = random.randint(0, H - target_size)\n",
        "        left_t = random.randint(0, W - target_size)\n",
        "        target_block = image[:, top_t:top_t+target_size, left_t:left_t+target_size]\n",
        "        target_block = torch.nn.functional.interpolate(target_block.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
        "        target_blocks.append(target_block)\n",
        "    target_blocks = torch.stack(target_blocks)\n",
        "    return context_block, target_blocks, (top, left, context_size), None"
      ],
      "metadata": {
        "_uuid": "472ec271-36da-4330-89d9-beef8fa03a65",
        "_cell_guid": "88bb8d1b-8f77-4ef0-9561-13154dfc48ed",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "fB1mtdS58qKz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class IJEPADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.context_scale = context_scale\n",
        "        self.target_scale = target_scale\n",
        "        self.num_targets = num_targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.base_dataset[idx]\n",
        "        context_block, target_blocks, _, _ = extract_blocks(image, self.context_scale, self.target_scale, self.num_targets)\n",
        "        return context_block, target_blocks, label"
      ],
      "metadata": {
        "_uuid": "772f2dc6-6685-4a8d-beda-dc9089423fbf",
        "_cell_guid": "208fc17d-fc86-444d-9d21-464f592a9cd0",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "zv6kmoK88qK0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ijepa_dataset = IJEPADataset(train_dataset)\n",
        "val_ijepa_dataset   = IJEPADataset(val_dataset)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ijepa_dataset, batch_size=32, shuffle=False)\n",
        "val_loader   = DataLoader(val_ijepa_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "_uuid": "67fe58d3-5076-436c-99b4-5350a172b545",
        "_cell_guid": "db9f91fa-4c3f-4929-a2d0-2569fce8e7d6",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4TjuwcOV8qK0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Direct Loading vs Preprossed blocks"
      ],
      "metadata": {
        "id": "IKla3laPxTdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 100\n",
        "extract_times = []\n",
        "for i in range(n_samples):\n",
        "    img, label = train_dataset[i]  # Assumes train_base_dataset is already defined.\n",
        "    start = time.perf_counter()\n",
        "    # Call the extraction function (adjust parameters as needed)\n",
        "    context_block, target_blocks, _, _ = extract_blocks(img, context_scale=0.85, target_scale=0.2, num_targets=4)\n",
        "    end = time.perf_counter()\n",
        "    extract_times.append(end - start)\n",
        "\n",
        "avg_extract_time = sum(extract_times) / len(extract_times)\n",
        "print(f\"Average extract_blocks time over {n_samples} samples: {avg_extract_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "zQmM4ioFxa_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import os\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "import concurrent.futures\n",
        "import functools\n",
        "\n",
        "# Set device for GPU acceleration if available.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def extract_blocks(image, context_scale=0.85, target_scale=0.2, num_targets=4, max_overlap=0.5):\n",
        "    # Extract a central context block.\n",
        "    _, H, W = image.shape\n",
        "    context_size = int(context_scale * H)\n",
        "    top = (H - context_size) // 2\n",
        "    left = (W - context_size) // 2\n",
        "    context_block = image[:, top:top+context_size, left:left+context_size]\n",
        "    context_block = torch.nn.functional.interpolate(\n",
        "        context_block.unsqueeze(0),\n",
        "        size=(224, 224),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    ).squeeze(0)\n",
        "\n",
        "    # Extract num_targets target blocks randomly.\n",
        "    target_blocks = []\n",
        "    for _ in range(num_targets):\n",
        "        target_size = int(target_scale * H)\n",
        "        top_t = random.randint(0, H - target_size)\n",
        "        left_t = random.randint(0, W - target_size)\n",
        "        target_block = image[:, top_t:top_t+target_size, left_t:left_t+target_size]\n",
        "        target_block = torch.nn.functional.interpolate(\n",
        "            target_block.unsqueeze(0),\n",
        "            size=(224, 224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze(0)\n",
        "        target_blocks.append(target_block)\n",
        "    target_blocks = torch.stack(target_blocks)\n",
        "    return context_block, target_blocks, (top, left, context_size), None\n",
        "\n",
        "def process_sample(args, context_scale, target_scale, num_targets):\n",
        "    sample, image_path = args\n",
        "    img, label = sample\n",
        "    # Move image to GPU if available.\n",
        "    img = img.to(device)\n",
        "    context_block, target_blocks, _, _ = extract_blocks(img, context_scale, target_scale, num_targets)\n",
        "    # Bring results back to CPU and return along with label and original image path.\n",
        "    print(f\"image_path:{image_path}\")\n",
        "    return (context_block.cpu(), target_blocks.cpu(), label, image_path)\n",
        "\n",
        "class PrecomputedIJEPADataset(Dataset):\n",
        "    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4, cache_file=None):\n",
        "        self.cache_file = cache_file\n",
        "        if cache_file and os.path.exists(cache_file):\n",
        "            # Load precomputed data from disk.\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "        else:\n",
        "            # Create a list of samples along with their original image paths (if available).\n",
        "            if hasattr(base_dataset, 'samples'):\n",
        "                #base_samples = [(base_dataset[i], base_dataset.samples[i][0]) for i in range(len(base_dataset))]\n",
        "                base_samples = [(base_dataset[i], base_dataset.samples[i][0]) for i in tqdm(range(len(base_dataset)))]\n",
        "            else:\n",
        "                base_samples = [(sample, None) for sample in base_dataset]\n",
        "            print(f\"Processing {len(base_samples)} samples...\")\n",
        "            partial_process_sample = functools.partial(\n",
        "                process_sample,\n",
        "                context_scale=context_scale,\n",
        "                target_scale=target_scale,\n",
        "                num_targets=num_targets\n",
        "            )\n",
        "            # Submit tasks for each sample and store the corresponding future with its index.\n",
        "            with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
        "                futures = {executor.submit(partial_process_sample, sample): idx for idx, sample in enumerate(base_samples)}\n",
        "                results = [None] * len(base_samples)\n",
        "                # Iterate over completed futures with a progress bar.\n",
        "                for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing samples\"):\n",
        "                    idx = futures[future]\n",
        "                    results[idx] = future.result()\n",
        "                self.data = results\n",
        "            # Use 8 CPU cores for parallel processing with a progress bar.\n",
        "            #with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
        "            #    results = list(tqdm(executor.map(partial_process_sample, base_samples),\n",
        "            #                        total=len(base_samples),\n",
        "            #                        desc=\"Processing samples\"))\n",
        "                self.data = results\n",
        "        if cache_file:\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(self.data, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "'''"
      ],
      "metadata": {
        "id": "n5MKyh1loxFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE6-L2V79Ej-",
        "outputId": "fd0f4ba4-e03e-40c1-f03f-3c10dcd86440"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AAVDATASET/spectrogram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Assume extract_blocks and train_base_dataset are defined.\n",
        "\n",
        "class PrecomputedIJEPADataset(Dataset):\n",
        "    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4, cache_file=None):\n",
        "        self.cache_file = cache_file\n",
        "        if cache_file and os.path.exists(cache_file):\n",
        "            # Load precomputed data from disk.\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "        else:\n",
        "            self.data = []\n",
        "            for img, label in base_dataset:\n",
        "                context_block, target_blocks, _, _ = extract_blocks(img, context_scale, target_scale, num_targets)\n",
        "                self.data.append((context_block, target_blocks, label))\n",
        "            if cache_file:\n",
        "                with open(cache_file, 'wb') as f:\n",
        "                    pickle.dump(self.data, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Timing the loading of the dataset and DataLoader\n",
        "cache_path = os.path.join(base_dir,\"precomputed_fulldataset_aav.pkl\")\n",
        "\n",
        "start_time = time.time()\n",
        "dataset_aav_ijepa = PrecomputedIJEPADataset(dataset, cache_file=cache_path)\n",
        "end_time_train_ijepa_dataset = time.time()\n",
        "dataloader_aav_ijepa = DataLoader(train_ijepa_dataset, batch_size=32, shuffle=True)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken to load dataset: {end_time_train_ijepa_dataset - start_time:.4f} seconds and DataLoader: {end_time - start_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "aCLTXWwhysQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for _, _, label in train_ijepa_dataset:\n",
        "    counter[label] += 1\n",
        "\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "D4QkFEExZw4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pickle\n",
        "import os\n",
        "# ----- Measure cache file reading time -----\n",
        "cache_path = 'precomputed_aav.pkl'\n",
        "read_times = []\n",
        "n_reads = 10  # Number of times to load the cache file.\n",
        "\n",
        "for i in range(n_reads):\n",
        "    start = time.perf_counter()\n",
        "    with open(cache_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    end = time.perf_counter()\n",
        "    read_times.append(end - start)\n",
        "\n",
        "avg_read_time = sum(read_times) / len(read_times)\n",
        "print(f\"Average time to load cache file over {n_reads} runs: {avg_read_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "_x7wLJhOyV0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Timing the loading of the dataset and DataLoader\n",
        "cache_path = 'precomputed_aav.pkl'\n",
        "start_time = time.time()\n",
        "\n",
        "train_ijepa_dataset = PrecomputedIJEPADataset(train_dataset, cache_file=cache_path)\n",
        "end_time_train_ijepa_dataset = time.time()\n",
        "train_loader = DataLoader(train_ijepa_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken to load dataset: {end_time_train_ijepa_dataset - start_time:.4f} seconds and DataLoader: {end_time - start_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "PCVlnbMXGDRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 100\n",
        "extract_times = []\n",
        "for i in range(n_samples):\n",
        "    start = time.perf_counter()\n",
        "    context_block, target_blocks, label = train_ijepa_dataset[i]  # Assumes train_base_dataset is already defined.\n",
        "\n",
        "    # Call the extraction function (adjust parameters as needed)\n",
        "    #context_block, target_blocks, _, _ = extract_blocks(img, context_scale=0.85, target_scale=0.2, num_targets=4)\n",
        "    end = time.perf_counter()\n",
        "    extract_times.append(end - start)\n",
        "\n",
        "avg_extract_time = sum(extract_times) / len(extract_times)\n",
        "print(f\"Average extract_blocks time over {n_samples} samples: {avg_extract_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "hp6aB0qzGzTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vit_encoder():\n",
        "    model = vit_b_16(pretrained=False)\n",
        "    model.heads = nn.Identity()  # remove classification head\n",
        "    return model"
      ],
      "metadata": {
        "_uuid": "f3ad3daa-80ba-4ae8-8b4e-770815bad02e",
        "_cell_guid": "7aeb9d44-2fd8-4da1-87c0-4b992adb53db",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "YQIK4mND8qK1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context_encoder = get_vit_encoder().cuda()\n",
        "target_encoder  = get_vit_encoder().cuda()\n",
        "target_encoder.load_state_dict(context_encoder.state_dict())"
      ],
      "metadata": {
        "_uuid": "b79e21b1-3b97-4dfc-958f-bddb3c94c746",
        "_cell_guid": "2d8198b7-dbf5-4228-975a-8c733cea1a07",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "rkq_fT7g8qK1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=768, output_dim=768, num_targets=4):\n",
        "        super().__init__()\n",
        "        self.num_targets = num_targets\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim * num_targets)\n",
        "        )\n",
        "    def forward(self, context_repr):\n",
        "        pred = self.mlp(context_repr)\n",
        "        # Reshape to [B, num_targets, output_dim]\n",
        "        return pred.view(-1, self.num_targets, pred.size(-1) // self.num_targets)"
      ],
      "metadata": {
        "_uuid": "e303f4ac-a09b-4f56-b997-6b364d63a675",
        "_cell_guid": "9206cc42-0845-4604-ab30-6852e28e2dcb",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "OApD7Fy68qK1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Set up optimizer, loss, and EMA update (same as your CIFAR code).\n",
        "predictor = Predictor().cuda()\n",
        "optimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "ema_decay = 0.99"
      ],
      "metadata": {
        "_uuid": "66d2337f-fb52-4652-9da7-242ce9d9d5e1",
        "_cell_guid": "e10ef302-1659-4a97-9bbb-2de90ac902c9",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZsZNIdEk8qK2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def update_ema(model, model_ema, beta):\n",
        "    for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n",
        "        param_ema.data.mul_(beta).add_(param.data, alpha=1 - beta)"
      ],
      "metadata": {
        "_uuid": "0e90599a-d03d-4231-a58a-1272a3613cc3",
        "_cell_guid": "aece458a-d1da-4334-804f-b054cf8ed0fc",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "kM60lfOw8qK2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for visualizations if it doesn't exist.\n",
        "#viz_dir = \"/kaggle/working/viz\"\n",
        "#os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "num_epochs = 100\n",
        "ema_decay = 0.99\n",
        "best_loss = float('inf')\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    context_encoder.train()\n",
        "    predictor.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Enumerate over batches with a progress bar.\n",
        "    for batch_idx, (context_block, target_blocks, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "        context_block = context_block.cuda()            # [B, C, 224, 224]\n",
        "        target_blocks = target_blocks.cuda()              # [B, num_targets, C, 224, 224]\n",
        "\n",
        "        # Forward pass through context encoder and predictor.\n",
        "        context_repr = context_encoder(context_block)     # [B, 768]\n",
        "        preds = predictor(context_repr)                   # [B, num_targets, 768]\n",
        "\n",
        "        B, num_targets, C, Ht, Wt = target_blocks.shape\n",
        "        target_blocks_flat = target_blocks.view(B * num_targets, C, Ht, Wt)\n",
        "        with torch.no_grad():\n",
        "            target_repr_flat = target_encoder(target_blocks_flat)\n",
        "        target_repr = target_repr_flat.view(B, num_targets, -1)\n",
        "\n",
        "        loss = criterion(preds, target_repr)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        update_ema(context_encoder, target_encoder, ema_decay)\n",
        "        running_loss += loss.item() * context_block.size(0)\n",
        "\n",
        "        # --- Visualization for first image of the current batch ---\n",
        "        '''with torch.no_grad():\n",
        "            # Get the first sample's context block and compute its feature vector.\n",
        "            context_img = context_block[0].cpu()  # shape: [C, 224, 224]\n",
        "            context_feat = context_encoder(context_block[0].unsqueeze(0)).cpu().squeeze(0)  # shape: [768]\n",
        "            # Reshape feature vector to a 2D heatmap (24x32).\n",
        "            context_heat = context_feat.view(24, 32).numpy()\n",
        "\n",
        "            # For target, choose the first target block of the first sample.\n",
        "            target_img = target_blocks[0][0].cpu()  # shape: [C, 224, 224]\n",
        "            target_feat = target_encoder(target_blocks[0][0].unsqueeze(0).to(context_block.device)).cpu().squeeze(0)\n",
        "            target_heat = target_feat.view(24, 32).numpy()\n",
        "\n",
        "            # Plot the images and corresponding heatmaps.\n",
        "            fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "            # Display context block image.\n",
        "            if context_img.shape[0] == 1:\n",
        "                axs[0, 0].imshow(context_img.squeeze(), cmap='gray')\n",
        "            else:\n",
        "                axs[0, 0].imshow(context_img.permute(1, 2, 0))\n",
        "            axs[0, 0].set_title(\"Context Block\")\n",
        "            axs[0, 0].axis(\"off\")\n",
        "\n",
        "            # Display context feature heatmap.\n",
        "            im0 = axs[0, 1].imshow(context_heat, cmap=\"viridis\")\n",
        "            axs[0, 1].set_title(\"Context Feature Heatmap\")\n",
        "            axs[0, 1].axis(\"off\")\n",
        "            fig.colorbar(im0, ax=axs[0, 1])\n",
        "\n",
        "            # Display target block image.\n",
        "            if target_img.shape[0] == 1:\n",
        "                axs[1, 0].imshow(target_img.squeeze(), cmap='gray')\n",
        "            else:\n",
        "                axs[1, 0].imshow(target_img.permute(1, 2, 0))\n",
        "            axs[1, 0].set_title(\"Target Block\")\n",
        "            axs[1, 0].axis(\"off\")\n",
        "\n",
        "            # Display target feature heatmap.\n",
        "            im1 = axs[1, 1].imshow(target_heat, cmap=\"viridis\")\n",
        "            axs[1, 1].set_title(\"Target Feature Heatmap\")\n",
        "            axs[1, 1].axis(\"off\")\n",
        "            fig.colorbar(im1, ax=axs[1, 1])\n",
        "\n",
        "            # Save the visualization figure with epoch and batch number.\n",
        "            viz_path = os.path.join(viz_dir, f\"epoch{epoch+1}_batch{batch_idx+1}.png\")\n",
        "            plt.savefig(viz_path)\n",
        "            plt.close(fig)'''\n",
        "\n",
        "    epoch_loss = running_loss / len(train_ijepa_dataset)\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.10f} - Epoch Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save checkpoint if current epoch loss is lower than previous best.\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'context_encoder_state_dict': context_encoder.state_dict(),\n",
        "            'target_encoder_state_dict': target_encoder.state_dict(),\n",
        "            'predictor_state_dict': predictor.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss\n",
        "        }\n",
        "        torch.save(checkpoint, \"/content/drive/MyDrive/AAVDATASET/spectrogram/ijepa_checkpoint_best.pth\")\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "total_train_time = time.time() - total_start_time\n",
        "print(f\"Total Training Time: {total_train_time:.2f}s\")"
      ],
      "metadata": {
        "_uuid": "c8659b2c-5286-4027-b97b-c1ac887bb5f6",
        "_cell_guid": "0182b713-044f-4839-9cfc-0369f1eec96d",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IjnBW1lE8qK2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search EMA Decay"
      ],
      "metadata": {
        "id": "0iwRsWF2Jam_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define candidate EMA decay values.\n",
        "ema_decay_candidates = [0.1, 0.3, 0.5, 0.7, 0.95, 0.97, 0.99, 0.999]\n",
        "num_epochs_grid = 10  # Use fewer epochs for grid search.\n",
        "grid_results = {}      # To record the best loss for each candidate.\n",
        "training_curves = {}   # To store epoch loss curves for each candidate.\n",
        "\n",
        "# Loop over each EMA decay candidate.\n",
        "for candidate in ema_decay_candidates:\n",
        "    print(f\"\\nTraining with ema_decay = {candidate}\")\n",
        "\n",
        "    # Reinitialize models for each candidate.\n",
        "    context_encoder_candidate = get_vit_encoder().cuda()\n",
        "    target_encoder_candidate = get_vit_encoder().cuda()\n",
        "    target_encoder_candidate.load_state_dict(context_encoder_candidate.state_dict())\n",
        "    predictor_candidate = Predictor().cuda()\n",
        "\n",
        "    # Set up a new optimizer.\n",
        "    optimizer_candidate = torch.optim.Adam(\n",
        "        list(context_encoder_candidate.parameters()) + list(predictor_candidate.parameters()),\n",
        "        lr=1e-3\n",
        "    )\n",
        "\n",
        "    best_loss_candidate = float('inf')\n",
        "    candidate_losses = []  # To store epoch losses for this candidate.\n",
        "\n",
        "    for epoch in range(num_epochs_grid):\n",
        "        context_encoder_candidate.train()\n",
        "        predictor_candidate.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        for batch_idx, (context_block, target_blocks, _) in enumerate(\n",
        "            tqdm(train_loader, desc=f\"Candidate {candidate} Epoch {epoch+1}/{num_epochs_grid}\", leave=False)\n",
        "        ):\n",
        "            context_block = context_block.cuda()            # [B, C, 224, 224]\n",
        "            target_blocks = target_blocks.cuda()              # [B, num_targets, C, 224, 224]\n",
        "\n",
        "            # Forward pass.\n",
        "            context_repr = context_encoder_candidate(context_block)  # [B, 768]\n",
        "            preds = predictor_candidate(context_repr)                # [B, num_targets, 768]\n",
        "\n",
        "            B, num_targets, C, Ht, Wt = target_blocks.shape\n",
        "            target_blocks_flat = target_blocks.view(B * num_targets, C, Ht, Wt)\n",
        "            with torch.no_grad():\n",
        "                target_repr_flat = target_encoder_candidate(target_blocks_flat)\n",
        "            target_repr = target_repr_flat.view(B, num_targets, -1)\n",
        "\n",
        "            loss = criterion(preds, target_repr)\n",
        "            optimizer_candidate.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_candidate.step()\n",
        "\n",
        "            # Update target encoder with the candidate's EMA decay.\n",
        "            update_ema(context_encoder_candidate, target_encoder_candidate, candidate)\n",
        "\n",
        "            running_loss += loss.item() * context_block.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_ijepa_dataset)\n",
        "        candidate_losses.append(epoch_loss)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        print(f\"ema_decay={candidate}, Epoch {epoch+1}/{num_epochs_grid} - Loss: {epoch_loss:.10f} - Epoch Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        if epoch_loss < best_loss_candidate:\n",
        "            best_loss_candidate = epoch_loss\n",
        "\n",
        "    training_curves[candidate] = candidate_losses\n",
        "    grid_results[candidate] = best_loss_candidate\n",
        "    print(f\"Candidate ema_decay = {candidate} achieved best loss: {best_loss_candidate:.10f}\")\n",
        "\n",
        "# Print grid search results.\n",
        "print(\"\\nGrid Search Results:\")\n",
        "for candidate, loss in grid_results.items():\n",
        "    print(f\"ema_decay {candidate}: Best Loss = {loss:.10f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Plot learning curves for all EMA decay candidates.\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "epochs = range(1, num_epochs_grid + 1)\n",
        "for candidate, losses in training_curves.items():\n",
        "    plt.plot(epochs, losses, marker='o', label=f\"ema_decay = {candidate}\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"EMA Decay Grid Search - Training Loss Curves\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/AAVDATASET/spectrogram/ema_decay_grid_search.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h3j3XL5yJlI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search of learning rate for the best EMA decay"
      ],
      "metadata": {
        "id": "ydZM4o4DZen6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define candidate learning rate values.\n",
        "lr_candidates = [1e-4, 1e-3, 1e-2, 1e-1]\n",
        "num_epochs_grid = 10  # Use a smaller number of epochs for the grid search.\n",
        "grid_results = {}      # To record the best loss for each candidate.\n",
        "training_curves = {}   # To store per-epoch loss curves for each candidate.\n",
        "\n",
        "# Loop over each learning rate candidate.\n",
        "for lr in lr_candidates:\n",
        "    print(f\"\\nTraining with learning rate = {lr}\")\n",
        "\n",
        "    # Reinitialize models for each candidate.\n",
        "    context_encoder_candidate = get_vit_encoder().cuda()\n",
        "    target_encoder_candidate = get_vit_encoder().cuda()\n",
        "    target_encoder_candidate.load_state_dict(context_encoder_candidate.state_dict())\n",
        "    predictor_candidate = Predictor().cuda()\n",
        "\n",
        "    # Set up a new optimizer with the current learning rate.\n",
        "    optimizer_candidate = torch.optim.Adam(\n",
        "        list(context_encoder_candidate.parameters()) + list(predictor_candidate.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    best_loss_candidate = float('inf')\n",
        "    candidate_losses = []  # To store the epoch loss for this candidate.\n",
        "\n",
        "    for epoch in range(num_epochs_grid):\n",
        "        context_encoder_candidate.train()\n",
        "        predictor_candidate.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        for batch_idx, (context_block, target_blocks, _) in enumerate(\n",
        "            tqdm(train_loader, desc=f\"LR {lr} Epoch {epoch+1}/{num_epochs_grid}\", leave=False)\n",
        "        ):\n",
        "            context_block = context_block.cuda()            # [B, C, 224, 224]\n",
        "            target_blocks = target_blocks.cuda()              # [B, num_targets, C, 224, 224]\n",
        "\n",
        "            # Forward pass.\n",
        "            context_repr = context_encoder_candidate(context_block)  # [B, 768]\n",
        "            preds = predictor_candidate(context_repr)                # [B, num_targets, 768]\n",
        "\n",
        "            B, num_targets, C, Ht, Wt = target_blocks.shape\n",
        "            target_blocks_flat = target_blocks.view(B * num_targets, C, Ht, Wt)\n",
        "            with torch.no_grad():\n",
        "                target_repr_flat = target_encoder_candidate(target_blocks_flat)\n",
        "            target_repr = target_repr_flat.view(B, num_targets, -1)\n",
        "\n",
        "            loss = criterion(preds, target_repr)\n",
        "            optimizer_candidate.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_candidate.step()\n",
        "\n",
        "            # For this grid search over learning rate, we keep the EMA update constant (e.g. using a fixed value).\n",
        "            update_ema(context_encoder_candidate, target_encoder_candidate, 0.1)\n",
        "\n",
        "            running_loss += loss.item() * context_block.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_ijepa_dataset)\n",
        "        candidate_losses.append(epoch_loss)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        print(f\"LR {lr}, Epoch {epoch+1}/{num_epochs_grid} - Loss: {epoch_loss:.15f} - Epoch Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        if epoch_loss < best_loss_candidate:\n",
        "            best_loss_candidate = epoch_loss\n",
        "\n",
        "    training_curves[lr] = candidate_losses\n",
        "    grid_results[lr] = best_loss_candidate\n",
        "    print(f\"Candidate lr = {lr} achieved best loss: {best_loss_candidate:.15f}\")\n",
        "\n",
        "# Print grid search results.\n",
        "print(\"\\nGrid Search Results:\")\n",
        "for candidate, loss in grid_results.items():\n",
        "    print(f\"Learning Rate {candidate}: Best Loss = {loss:.15f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Plot learning curves for all learning rate candidates.\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "epochs = range(1, num_epochs_grid + 1)\n",
        "for lr, losses in training_curves.items():\n",
        "    plt.plot(epochs, losses, marker='o', label=f\"lr = {lr}\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Learning Rate Grid Search - Training Loss Curves\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/AAVDATASET/spectrogram/learning_rate_grid_search.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qIXX-3rXZkHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Classifier"
      ],
      "metadata": {
        "id": "clZWxsbpZ9EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the Saved Checkpoint for the Self-Supervised Model"
      ],
      "metadata": {
        "id": "GQnzVW9aaDL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(os.path.join(base_dir, \"ijepa_checkpoint_best.pth\"))\n",
        "context_encoder.load_state_dict(checkpoint['context_encoder_state_dict'])\n",
        "# Freeze the context encoder.\n",
        "context_encoder.eval()\n",
        "for param in context_encoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "TXG_-3UPaHTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the Classifier"
      ],
      "metadata": {
        "id": "W4p4tYAraGVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3  # Adjust this number based on your dataset.\n",
        "classifier = nn.Linear(768, num_classes).cuda()\n"
      ],
      "metadata": {
        "id": "rGFXEjdsaOW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Set Up Optimizer and Loss Criterion"
      ],
      "metadata": {
        "id": "X0ZKbi8UaQig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
        "criterion_cls = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "R19laeJNaTp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Loop for the Classifier (Using Training Data Only)"
      ],
      "metadata": {
        "id": "1L2khmV-aWGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs_clf = 500\n",
        "best_train_acc = 0.0  # Best training accuracy so far.\n",
        "\n",
        "for epoch in range(num_epochs_clf):\n",
        "    epoch_start_time = time.time()\n",
        "    classifier.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for context_block, _, label in train_loader:\n",
        "        context_block = context_block.cuda()  # [B, C, 224, 224]\n",
        "        label = label.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = context_encoder(context_block)  # [B, 768]\n",
        "\n",
        "        logits = classifier(features)  # [B, num_classes]\n",
        "        loss = criterion_cls(logits, label)\n",
        "\n",
        "        clf_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clf_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * context_block.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_train += (preds == label).sum().item()\n",
        "        total_train += label.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_ijepa_dataset)\n",
        "    epoch_train_acc = correct_train / total_train\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_clf} - Train Loss: {epoch_train_loss:.10f} | Train Acc: {epoch_train_acc*100:.10f}% | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save checkpoint if training accuracy improves.\n",
        "    if epoch_train_acc > best_train_acc:\n",
        "        best_train_acc = epoch_train_acc\n",
        "        checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'classifier_state_dict': classifier.state_dict(),\n",
        "            'optimizer_state_dict': clf_optimizer.state_dict(),\n",
        "            'train_loss': epoch_train_loss,\n",
        "            'train_acc': epoch_train_acc\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(base_dir,\"ijepa_classifier_best.pth\"))\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1} with Train Acc: {epoch_train_acc*100:.10f}%\")\n",
        "\n",
        "print(\"Classifier training complete!\")"
      ],
      "metadata": {
        "id": "3nQoDWvwaZ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.eval_js('google.colab.kernel.disconnect()')"
      ],
      "metadata": {
        "id": "OfvaWDggmMKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "metadata": {
        "id": "TDqi8kpNuRWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}