{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyPzRU4Sa85efB68njv1CUKU","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11224133,"sourceType":"datasetVersion","datasetId":7010008}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import drive\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom collections import Counter\nimport torch.nn.functional as F\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim as optim\nimport time\nfrom tqdm import tqdm\nimport torch\nimport timm\nimport matplotlib.pyplot as plt\nimport copy\nfrom torchvision.models import vit_b_16\nfrom collections import Counter\nfrom tqdm import tqdm # for progress bar\nimport random\nimport os","metadata":{"_uuid":"ab0f765f-50d0-4e71-934b-69036f927618","_cell_guid":"1b74e769-17ce-492e-bbf7-eb90938bae5d","collapsed":false,"id":"7Qm5o5BpuTyt","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#drive.mount('/content/drive')","metadata":{"_uuid":"b02bcb6e-1d67-4b0c-9d11-a22f3ab07a61","_cell_guid":"d0758325-ed5a-4988-b20a-51539395a900","trusted":true,"collapsed":false,"id":"Gxnc6znruQbZ","outputId":"cd3e4062-2a7d-4ea5-bffc-08f8fe8d51b5","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = '/kaggle/input/aav-spectrogram/spectrogram'\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"_uuid":"2bb0d814-c433-4069-b2de-b8afb06f8c33","_cell_guid":"cfbd3054-08ff-4e37-a1dd-a9816ded260d","trusted":true,"collapsed":false,"id":"fKthbkn-t67J","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test loading\nimages, labels = next(iter(dataloader))\nprint(\"Batch shape:\", images.shape)  # Should be [B, C, 224, 224]","metadata":{"_uuid":"5d688a1b-2546-4240-bbb7-5aec8d108cfc","_cell_guid":"a8f3ba24-1e87-429a-bbee-4909d3832d58","trusted":true,"collapsed":false,"id":"C9kEg-mIKiF3","outputId":"139cb538-a806-4b6b-cd22-75e8785de5f5","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"_uuid":"ebcd8a59-646f-41d1-98a2-6111730750c6","_cell_guid":"577f43b2-192c-4d42-9928-3612f02483bc","trusted":true,"collapsed":false,"id":"N2As1-TPux9B","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nfor i in range(6):\n    img = images[i].permute(1, 2, 0) * 0.5 + 0.5  # Unnormalize\n    plt.subplot(2, 3, i+1)\n    plt.imshow(img.squeeze(), cmap='viridis')\n    plt.axis('off')\nplt.suptitle(\"Sample Spectrograms (Resized to 224Ã—224)\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"7b5e6218-b07f-4701-a372-28b857d114d6","_cell_guid":"551e8024-3177-4665-896d-27a3497990e0","trusted":true,"collapsed":false,"id":"rRDzaHnJKw9k","outputId":"99d5d47d-b3e1-4662-ae06-b13334f1100b","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef count_images_per_class(train_loader):\n    \"\"\"Counts the number of images per class in a train_loader.\n\n    Args:\n        train_loader: The DataLoader for the training dataset.\n\n    Returns:\n        A dictionary where keys are class labels and values are the corresponding counts.\n    \"\"\"\n\n    all_labels = []\n    for images, labels in tqdm(train_loader, desc=\"Counting images per class\"):\n        all_labels.extend(labels.tolist())  # Extend the list with labels from the batch\n\n    class_counts = Counter(all_labels)  # Count occurrences of each label\n\n    return class_counts","metadata":{"_uuid":"c9b49613-e5e4-4a6f-ae14-7287ae13e3a4","_cell_guid":"68672c2b-407b-42e6-a744-2523bf327831","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the counts and print them\n#class_counts = count_images_per_class(train_loader)\n\n#print(\"Images per class in train_loader:\")\n#for class_label, count in class_counts.items():\n#    print(f\"Class {class_label}: {count} images\")","metadata":{"_uuid":"3e31c353-e70e-4e37-a58f-1a4d5ffab8d1","_cell_guid":"46c3eb6b-6513-46a0-a732-946df9564d7e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the counts and print them\n#class_counts = count_images_per_class(val_loader)\n\n#print(\"Images per class in train_loader:\")\n#for class_label, count in class_counts.items():\n#    print(f\"Class {class_label}: {count} images\")","metadata":{"_uuid":"f66b9f67-0023-42ea-bea2-456f7146ab2e","_cell_guid":"a8f02f58-fcb6-4234-9407-45f1c6e3e11e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_blocks(image, context_scale=0.85, target_scale=0.2, num_targets=4, max_overlap=0.5):\n    # Extract a central context block.\n    _, H, W = image.shape\n    context_size = int(context_scale * H)\n    top = (H - context_size) // 2\n    left = (W - context_size) // 2\n    context_block = image[:, top:top+context_size, left:left+context_size]\n    context_block = torch.nn.functional.interpolate(context_block.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n    \n    # Extract num_targets target blocks randomly.\n    target_blocks = []\n    for _ in range(num_targets):\n        target_size = int(target_scale * H)\n        top_t = random.randint(0, H - target_size)\n        left_t = random.randint(0, W - target_size)\n        target_block = image[:, top_t:top_t+target_size, left_t:left_t+target_size]\n        target_block = torch.nn.functional.interpolate(target_block.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n        target_blocks.append(target_block)\n    target_blocks = torch.stack(target_blocks)\n    return context_block, target_blocks, (top, left, context_size), None","metadata":{"_uuid":"472ec271-36da-4330-89d9-beef8fa03a65","_cell_guid":"88bb8d1b-8f77-4ef0-9561-13154dfc48ed","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class IJEPADataset(torch.utils.data.Dataset):\n    def __init__(self, base_dataset, context_scale=0.85, target_scale=0.2, num_targets=4):\n        self.base_dataset = base_dataset\n        self.context_scale = context_scale\n        self.target_scale = target_scale\n        self.num_targets = num_targets\n        \n    def __len__(self):\n        return len(self.base_dataset)\n    \n    def __getitem__(self, idx):\n        image, label = self.base_dataset[idx]\n        context_block, target_blocks, _, _ = extract_blocks(image, self.context_scale, self.target_scale, self.num_targets)\n        return context_block, target_blocks, label","metadata":{"_uuid":"772f2dc6-6685-4a8d-beda-dc9089423fbf","_cell_guid":"208fc17d-fc86-444d-9d21-464f592a9cd0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ijepa_dataset = IJEPADataset(train_dataset)\nval_ijepa_dataset   = IJEPADataset(val_dataset)\n\n\ntrain_loader = DataLoader(train_ijepa_dataset, batch_size=32, shuffle=False)\nval_loader   = DataLoader(val_ijepa_dataset, batch_size=32, shuffle=False)","metadata":{"_uuid":"67fe58d3-5076-436c-99b4-5350a172b545","_cell_guid":"db9f91fa-4c3f-4929-a2d0-2569fce8e7d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_vit_encoder():\n    model = vit_b_16(pretrained=False)\n    model.heads = nn.Identity()  # remove classification head\n    return model","metadata":{"_uuid":"f3ad3daa-80ba-4ae8-8b4e-770815bad02e","_cell_guid":"7aeb9d44-2fd8-4da1-87c0-4b992adb53db","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncontext_encoder = get_vit_encoder().cuda()\ntarget_encoder  = get_vit_encoder().cuda()\ntarget_encoder.load_state_dict(context_encoder.state_dict())","metadata":{"_uuid":"b79e21b1-3b97-4dfc-958f-bddb3c94c746","_cell_guid":"2d8198b7-dbf5-4228-975a-8c733cea1a07","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Predictor(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, output_dim=768, num_targets=4):\n        super().__init__()\n        self.num_targets = num_targets\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim * num_targets)\n        )\n    def forward(self, context_repr):\n        pred = self.mlp(context_repr)\n        # Reshape to [B, num_targets, output_dim]\n        return pred.view(-1, self.num_targets, pred.size(-1) // self.num_targets)","metadata":{"_uuid":"e303f4ac-a09b-4f56-b997-6b364d63a675","_cell_guid":"9206cc42-0845-4604-ab30-6852e28e2dcb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Set up optimizer, loss, and EMA update (same as your CIFAR code).\npredictor = Predictor().cuda()\noptimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=1e-3)\ncriterion = nn.MSELoss()\nema_decay = 0.99","metadata":{"_uuid":"66d2337f-fb52-4652-9da7-242ce9d9d5e1","_cell_guid":"e10ef302-1659-4a97-9bbb-2de90ac902c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef update_ema(model, model_ema, beta):\n    for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n        param_ema.data.mul_(beta).add_(param.data, alpha=1 - beta)","metadata":{"_uuid":"0e90599a-d03d-4231-a58a-1272a3613cc3","_cell_guid":"aece458a-d1da-4334-804f-b054cf8ed0fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a directory for visualizations if it doesn't exist.\nviz_dir = \"/kaggle/working/viz\"\nos.makedirs(viz_dir, exist_ok=True)\n\nnum_epochs = 200\nema_decay = 0.99\nbest_loss = float('inf')\ntotal_start_time = time.time()\n\nfor epoch in range(num_epochs):\n    epoch_start_time = time.time()\n    context_encoder.train()\n    predictor.train()\n    running_loss = 0.0\n    \n    # Enumerate over batches with a progress bar.\n    for batch_idx, (context_block, target_blocks, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n        context_block = context_block.cuda()            # [B, C, 224, 224]\n        target_blocks = target_blocks.cuda()              # [B, num_targets, C, 224, 224]\n        \n        # Forward pass through context encoder and predictor.\n        context_repr = context_encoder(context_block)     # [B, 768]\n        preds = predictor(context_repr)                   # [B, num_targets, 768]\n        \n        B, num_targets, C, Ht, Wt = target_blocks.shape\n        target_blocks_flat = target_blocks.view(B * num_targets, C, Ht, Wt)\n        with torch.no_grad():\n            target_repr_flat = target_encoder(target_blocks_flat)\n        target_repr = target_repr_flat.view(B, num_targets, -1)\n        \n        loss = criterion(preds, target_repr)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        update_ema(context_encoder, target_encoder, ema_decay)\n        running_loss += loss.item() * context_block.size(0)\n        \n        # --- Visualization for first image of the current batch ---\n        '''with torch.no_grad():\n            # Get the first sample's context block and compute its feature vector.\n            context_img = context_block[0].cpu()  # shape: [C, 224, 224]\n            context_feat = context_encoder(context_block[0].unsqueeze(0)).cpu().squeeze(0)  # shape: [768]\n            # Reshape feature vector to a 2D heatmap (24x32).\n            context_heat = context_feat.view(24, 32).numpy()\n            \n            # For target, choose the first target block of the first sample.\n            target_img = target_blocks[0][0].cpu()  # shape: [C, 224, 224]\n            target_feat = target_encoder(target_blocks[0][0].unsqueeze(0).to(context_block.device)).cpu().squeeze(0)\n            target_heat = target_feat.view(24, 32).numpy()\n            \n            # Plot the images and corresponding heatmaps.\n            fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n            \n            # Display context block image.\n            if context_img.shape[0] == 1:\n                axs[0, 0].imshow(context_img.squeeze(), cmap='gray')\n            else:\n                axs[0, 0].imshow(context_img.permute(1, 2, 0))\n            axs[0, 0].set_title(\"Context Block\")\n            axs[0, 0].axis(\"off\")\n            \n            # Display context feature heatmap.\n            im0 = axs[0, 1].imshow(context_heat, cmap=\"viridis\")\n            axs[0, 1].set_title(\"Context Feature Heatmap\")\n            axs[0, 1].axis(\"off\")\n            fig.colorbar(im0, ax=axs[0, 1])\n            \n            # Display target block image.\n            if target_img.shape[0] == 1:\n                axs[1, 0].imshow(target_img.squeeze(), cmap='gray')\n            else:\n                axs[1, 0].imshow(target_img.permute(1, 2, 0))\n            axs[1, 0].set_title(\"Target Block\")\n            axs[1, 0].axis(\"off\")\n            \n            # Display target feature heatmap.\n            im1 = axs[1, 1].imshow(target_heat, cmap=\"viridis\")\n            axs[1, 1].set_title(\"Target Feature Heatmap\")\n            axs[1, 1].axis(\"off\")\n            fig.colorbar(im1, ax=axs[1, 1])\n            \n            # Save the visualization figure with epoch and batch number.\n            viz_path = os.path.join(viz_dir, f\"epoch{epoch+1}_batch{batch_idx+1}.png\")\n            plt.savefig(viz_path)\n            plt.close(fig)'''\n    \n    epoch_loss = running_loss / len(train_ijepa_dataset)\n    epoch_time = time.time() - epoch_start_time\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.8f} - Epoch Time: {epoch_time:.2f}s\")\n    \n    # Save checkpoint if current epoch loss is lower than previous best.\n    if epoch_loss < best_loss:\n        best_loss = epoch_loss\n        checkpoint = {\n            'epoch': epoch+1,\n            'context_encoder_state_dict': context_encoder.state_dict(),\n            'target_encoder_state_dict': target_encoder.state_dict(),\n            'predictor_state_dict': predictor.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': epoch_loss\n        }\n        torch.save(checkpoint, \"/kaggle/working/ijepa_checkpoint_best.pth\")\n        print(f\"Checkpoint saved at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n\ntotal_train_time = time.time() - total_start_time\nprint(f\"Total Training Time: {total_train_time:.2f}s\")","metadata":{"_uuid":"c8659b2c-5286-4027-b97b-c1ac887bb5f6","_cell_guid":"0182b713-044f-4839-9cfc-0369f1eec96d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}